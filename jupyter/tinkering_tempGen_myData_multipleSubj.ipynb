{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mne\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from mne.datasets import sample\n",
    "from mne.decoding import cross_val_multiscore, LinearModel, GeneralizingEstimator, Scaler, \\\n",
    "                         Vectorizer\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_score, StratifiedShuffleSplit, \\\n",
    "                                    RepeatedStratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "import argparse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class arguments():\n",
    "    SAVE_EPOCH_ROOT = '../../data/version5.2/preprocessed/epochs/'\n",
    "    SAVE_RESULT_ROOT = '../../results/tempGen/subjects_scores/'\n",
    "    cond_filter ='none' # {none,non_symm}\n",
    "    cond_block ='early' #{early,later}\n",
    "    cond_time = 'prestim' #{prestim,poststim}\n",
    "    cond_decoding = 'none' #{none,removeevoked,resampled}\n",
    "    subj_num = 1\n",
    "    applyBaseline_bool = 1\n",
    "    pre_tmin = -0.4\n",
    "    pre_tmax = 0.05\n",
    "    post_tmin = 0.05\n",
    "    post_tmax = 0.45\n",
    "    num_classes = 2\n",
    "    normalization_type = 'normal'# {normal,lstmPaper}\n",
    "    gen_rand_perm = 0\n",
    "    null_max_iter = 10000\n",
    "    loop_null_iter = 5\n",
    "    gen_decoder_scores = 1\n",
    "    n_splits = 5\n",
    "    random_state = 42 \n",
    "    max_iter = 10000\n",
    "    n_jobs = 1\n",
    "    scoring = 'roc_auc'\n",
    "    \n",
    "args = arguments()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read each subj and preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Reading and preparing epoch data to create each 4 grous and 2 pattern\n",
    "\"\"\"\n",
    "def read_prep_epochs(args):\n",
    "    if args.applyBaseline_bool:\n",
    "        filename_epoch = args.SAVE_EPOCH_ROOT + \\\n",
    "                         'epochs_sec_applyBaseline_subj%s-afterRejICA-epo.fif' \\\n",
    "                          %args.subj_num\n",
    "    else:\n",
    "        filename_epoch = args.SAVE_EPOCH_ROOT + \\\n",
    "                         'epochs_sec_subj%s-afterRejICA-epo.fif' \\\n",
    "                         %args.subj_num\n",
    "    epochs_orig = mne.read_epochs(filename_epoch, proj=True, preload=True,\n",
    "                                  verbose=None)\n",
    "    epochs = epochs_orig.copy()\n",
    "    subset = epochs['pred']['non'].copy()\n",
    "    subset = subset.pick_types(eeg=True)\n",
    "    if (args.cond_decoding=='removeevoked'):\n",
    "        # REMOVE EVOKED RESP.\n",
    "        subset.subtract_evoked()    # remove evoked response\n",
    "    elif (args.cond_decoding=='resampled'):\n",
    "        # RESAMPLE\n",
    "        subset = subset.resample(args.n_resampling, npad='auto')\n",
    "    else:\n",
    "        pass\n",
    "    ##==========================================================================\n",
    "    if subset['Block==7'].metadata.Ptrn_Type.values.shape[0]>0:\n",
    "       main_ptrn = subset['Block==7'].metadata.Ptrn_Type.values[0]\n",
    "    else:\n",
    "       main_ptrn = subset['Block==8'].metadata.Ptrn_Type.values[0]\n",
    "    ##==========================================================================\n",
    "    if args.cond_block=='early': #block 3-6\n",
    "        subset = subset['Block<7'].copy()\n",
    "        subset = subset['Block>2'].copy()\n",
    "    elif args.cond_block=='later':#block 7-10\n",
    "        subset = subset['Block<11'].copy()\n",
    "        subset = subset['Block>6'].copy()\n",
    "    ##==========================================================================\n",
    "    if (args.cond_time=='prestim'):\n",
    "        subset= subset.crop(tmin=-0.4, tmax=0.05)\n",
    "    if (args.cond_time=='poststim'):\n",
    "        subset= subset.crop(tmin=0.05, tmax=0.45)\n",
    "    print('Shape of data is\\n :')\n",
    "    print(subset._data.shape)\n",
    "    ##==========================================================================\n",
    "    # Group data based on the previous trial\n",
    "    Grp1 = subset['Trgt_Loc_prev==1'].copy()\n",
    "    Grp2 = subset['Trgt_Loc_prev==2'].copy()\n",
    "    Grp3 = subset['Trgt_Loc_prev==3'].copy()\n",
    "    Grp4 = subset['Trgt_Loc_prev==4'].copy()\n",
    "    if main_ptrn==1:\n",
    "        Grp1 = Grp1['Trgt_Loc_main!=4'].copy()\n",
    "        Grp2 = Grp2['Trgt_Loc_main!=1'].copy()\n",
    "        Grp3 = Grp3['Trgt_Loc_main!=2'].copy()\n",
    "        Grp4 = Grp4['Trgt_Loc_main!=3'].copy()\n",
    "    ##==========================================================================\n",
    "    frequencies = np.arange(3, 13, 2)\n",
    "    if args.cond_decoding=='non_symm':\n",
    "        Grp1 = apply_nonSymm_filter(Grp1, frequencies)\n",
    "        Grp2 = apply_nonSymm_filter(Grp2, frequencies)\n",
    "        Grp3 = apply_nonSymm_filter(Grp3, frequencies)\n",
    "        Grp4 = apply_nonSymm_filter(Grp4, frequencies)\n",
    "    ##==========================================================================\n",
    "    print('the pattern for this subj is :=====================================')\n",
    "    print(main_ptrn)\n",
    "    print('          ')\n",
    "    print('===================================================================')\n",
    "    ##==========================================================================\n",
    "    # Normalizing the data for each subject\n",
    "    if args.normalization_type=='normal':\n",
    "        Grp1._data = (Grp1._data - np.mean(Grp1._data)) / np.std(Grp1._data)\n",
    "        Grp2._data = (Grp2._data - np.mean(Grp2._data)) / np.std(Grp2._data)\n",
    "        Grp3._data = (Grp3._data - np.mean(Grp3._data)) / np.std(Grp3._data)\n",
    "        Grp4._data = (Grp4._data - np.mean(Grp4._data)) / np.std(Grp4._data)\n",
    "    elif args.normalization_type=='lstmPaper':\n",
    "        Grp1._data = (2 * (Grp1._data - np.min(Grp1._data))) \\\n",
    "                        / (np.max(Grp1._data) - np.min(Grp1._data) - 1)\n",
    "        Grp2._data = (2 * (Grp2._data - np.min(Grp2._data))) \\\n",
    "                        / (np.max(Grp2._data) - np.min(Grp2._data) - 1)\n",
    "        Grp3._data = (2 * (Grp3._data - np.min(Grp3._data))) \\\n",
    "                        / (np.max(Grp3._data) - np.min(Grp3._data) - 1)\n",
    "        Grp4._data = (2 * (Grp4._data - np.min(Grp4._data))) \\\n",
    "                        / (np.max(Grp4._data) - np.min(Grp4._data) - 1)\n",
    "    ##==========================================================================\n",
    "    return Grp1, Grp2, Grp3, Grp4, main_ptrn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function to set up a decoder and apply temporal generalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Temporal Generalization\n",
    "\"\"\"\n",
    "def apply_tempGen(args, Grp_data, cv):\n",
    "    le = LabelEncoder()\n",
    "    clf_SVC  = make_pipeline(\n",
    "                          StandardScaler(),\n",
    "                          LinearModel(LinearSVC(random_state=args.random_state,\n",
    "                                                max_iter=args.max_iter)))\n",
    "    X=Grp_data.copy()._data\n",
    "    y=le.fit_transform(Grp_data.copy().metadata.Trgt_Loc_main)\n",
    "\n",
    "    time_gen = GeneralizingEstimator(clf_SVC, scoring=args.scoring,\n",
    "                                     n_jobs=args.n_jobs, verbose=True)\n",
    "    print(np.unique(y))\n",
    "    print(np.unique(Grp_data.copy().metadata.Trgt_Loc_main))\n",
    "\n",
    "    scores = cross_val_multiscore(time_gen, X, y, cv=cv, n_jobs=args.n_jobs)\n",
    "    scores = np.mean(scores, axis=0) #scores with cv\n",
    "    scores_diag = np.diag(scores)\n",
    "    scores_pck = (scores.copy(), scores_diag.copy())\n",
    "\n",
    "    # Without using cv, train and test on the same data\n",
    "    X = Grp_data.copy()._data\n",
    "    y = le.fit_transform(Grp_data.copy().metadata.Trgt_Loc_main)\n",
    "    time_gen.fit(X=X ,y=y)\n",
    "    scores = time_gen.score(X=X, y=y) #scores without cv\n",
    "    scores_diag = np.diag(scores)\n",
    "    scores_pck_fit = (scores.copy(), scores_diag.copy())\n",
    "\n",
    "    return scores_pck, scores_pck_fit\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function to generate random data and apply decoders on them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_rand(args, Grp_data, cv):\n",
    "    le = LabelEncoder()\n",
    "    clf_SVC  = make_pipeline(\n",
    "                          StandardScaler(),\n",
    "                          LinearModel(LinearSVC(random_state=args.random_state,\n",
    "                                                max_iter=args.null_max_iter)))\n",
    "    \n",
    "    X = Grp_data.copy()._data\n",
    "    y = le.fit_transform(Grp_data.copy().metadata.Trgt_Loc_main)\n",
    "\n",
    "    rand_scores = []\n",
    "    rand_diag = []\n",
    "\n",
    "    rand_scores_fit = []\n",
    "    rand_diag_fit = []\n",
    "\n",
    "    for nitr in range(args.loop_null_iter):\n",
    "        true_Y = y.copy();\n",
    "        indx = np.random.permutation(true_Y.shape[0]);\n",
    "        shuffled_Y = true_Y.copy()[indx];\n",
    "        shuffled_Y = le.fit_transform(shuffled_Y.copy());\n",
    "\n",
    "        time_gen = GeneralizingEstimator(clf_SVC, scoring=args.scoring,\n",
    "                                         n_jobs=args.n_jobs, verbose=True)\n",
    "        print(np.unique(y))\n",
    "        print(np.unique(Grp_data.copy().metadata.Trgt_Loc_main))\n",
    "\n",
    "        scores = cross_val_multiscore(time_gen, X, shuffled_Y, cv=cv, n_jobs=args.n_jobs)\n",
    "        scores = np.mean(scores, axis=0) #scores with cv\n",
    "        scores_diag = np.diag(scores)\n",
    "        \n",
    "\n",
    "        rand_scores.append(scores)\n",
    "        rand_diag.append(scores_diag)\n",
    "\n",
    "        # Without using cv, train and test on the same data\n",
    "        X = Grp_data.copy()._data\n",
    "        y = le.fit_transform(Grp_data.copy().metadata.Trgt_Loc_main)\n",
    "        time_gen.fit(X=X ,y=y)\n",
    "        scores = time_gen.score(X=X, y=y) #scores without cv\n",
    "        scores_diag = np.diag(scores)\n",
    "        \n",
    "        rand_scores_fit.append(scores)\n",
    "        rand_diag_fit.append(scores_diag)\n",
    "\n",
    "    rand_scores_pck = (rand_scores.copy(), rand_diag.copy())\n",
    "    rand_scores_pck_fit = (rand_scores_fit.copy(), rand_diag_fit.copy())\n",
    "    \n",
    "    return rand_scores_pck, rand_scores_pck_fit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def smooth(y, window, mode):\n",
    "    box = np.ones(window)/window\n",
    "    y_smooth = np.convolve(y, box, mode=mode)\n",
    "    return y_smooth\n",
    "\n",
    "def plot_scores(scores):\n",
    "    fig, ax = plt.subplots(1, 1)\n",
    "    plt.tight_layout()\n",
    "    im = ax.imshow(scores, interpolation='lanczos', origin='lower', cmap='RdBu_r',\n",
    "                   extent=subset.times[[0, -1, 0 , -1]], vmin=0., vmax=1.)\n",
    "    ax.set_xlabel('Testing Time (s)')\n",
    "    ax.set_ylabel('Training Time (s)')\n",
    "    ax.set_title('Temporal generalization')\n",
    "    ax.axvline(0, color='k')\n",
    "    ax.axhline(0, color='k')\n",
    "    ax.xaxis.set_ticks_position('bottom')\n",
    "    plt.colorbar(im, ax=ax)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def plot_scores_diag(scores_diag, apply_smooth):\n",
    "    if apply_smooth:\n",
    "        window=50\n",
    "        mode='valid'\n",
    "        scores_diag = smooth(y, window, mode)\n",
    "        print(subset.times.shape)\n",
    "        print(y_smooth.shape)\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot(subset.times, scores_diag, label='score')\n",
    "    ax.axhline(.5, color='k', linestyle='--', label='chance')\n",
    "    ax.set_xlabel('Times')\n",
    "    ax.set_ylabel('AUC')  # Area Under the Curve\n",
    "    ax.legend()\n",
    "    ax.axvline(.0, color='k', linestyle='-')\n",
    "    ax.set_title('Sensor space decoding')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Statistical analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_time_bin(data, indx, sbt):\n",
    "    if sbt==0:\n",
    "        avgs=np.zeros(len(indx))\n",
    "        bs=np.array(np.split(data, indx))\n",
    "        for ii in range(len(indx)):\n",
    "            avgs[ii]=bs[ii].mean()\n",
    "    if sbt==1:\n",
    "         avgs=np.zeros([data.shape[0],len(indx)])\n",
    "         aa=np.zeros(len(indx))\n",
    "         for jj in range(data.shape[0]):\n",
    "             bs=np.array(np.split(data[jj,:], indx))\n",
    "             for ii in range(len(indx)):\n",
    "                 aa[ii]=bs[ii].mean()\n",
    "             avgs[jj,:]=aa\n",
    "    if sbt==2:\n",
    "         avgs=np.zeros([len(indx),len(indx)])\n",
    "         aa=np.zeros(len(indx))\n",
    "         for jj in range(data.shape[0]):\n",
    "             bs1=np.array(np.split(data[jj,:], indx))\n",
    "             bs2=np.array(np.split(data[:,jj], indx))\n",
    "             for ii in range(len(indx)):\n",
    "                 avgs[ii,:]=bs1[ii].mean()\n",
    "                 avgs[:,ii]=bs2[ii].mean()\n",
    "\n",
    "    return avgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_fonts():\n",
    "\n",
    "\n",
    "    font = FontProperties()\n",
    "    font.set_family('serif')\n",
    "    font.set_name('Calibri')\n",
    "    return font\n",
    "\n",
    "def plot_scores_stat(diag_scores, clusts):\n",
    "    font=set_fonts();\n",
    "    [t_obs, clusters, clusters_pv, H0] = clusts\n",
    "    # binned times\n",
    "    times=np.asarray([-0.4,-0.3,-0.2,-0.1,0.1,0.2,0.3,0.4])\n",
    "    extent_times=subset.times[[0, -1, 0, -1]]\n",
    "    \n",
    "    # Plot the diagonal (it's exactly the same as the time-by-time decoding above)\n",
    "    fig, ax = plt.subplots()\n",
    "    plt.tight_layout()\n",
    "    ax.plot(times, diag_scores, label='score')\n",
    "    ax.axhline(.5, color='k', linestyle='--', label='chance')\n",
    "    plt.ylim([0.43,0.65])\n",
    "    ax.axvline(.0, color='k', linestyle='-')\n",
    "\n",
    "    for i_clu, clu_idx in enumerate(clusters):\n",
    "        clu_idx=clu_idx[0]\n",
    "        print(clu_idx)\n",
    "        # unpack cluster information, get unique indices\n",
    "        if clusters_pv[i_clu] <= 0.05:\n",
    "            h = plt.axvspan(times[clu_idx[0]], times[clu_idx[-1] - 1],\n",
    "                            color='r', alpha=0.3)\n",
    "            plt.legend((h, ), ('cluster p-value < 0.05', ))\n",
    "        else:\n",
    "            plt.axvspan(times[clu_idx[0]], times[clu_idx[-1] - 1], color=(0.3, 0.3, 0.3),\n",
    "                        alpha=0.3)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.xlabel('Times',  fontproperties=font, fontsize=12, fontweight='bold')\n",
    "    plt.ylabel('AUC', fontproperties=font, fontsize=12, fontweight='bold')#, labelpad=16,)\n",
    "    plt.title('Decoding over time', fontproperties=font, fontweight='bold', fontsize=16)\n",
    "\n",
    "    plt.legend(fontsize=11)\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statistial analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stat_anal(scores_pck):\n",
    "    indx=[26,51,76,101,126,151,176,201]\n",
    "    \n",
    "    score, score_diag = scores_pck\n",
    "    score_subtract = score_diag - 0.5\n",
    "    \n",
    "    binned_score = do_time_bin(score, indx, 2)\n",
    "    binned_score_diag = do_time_bin(score_diag, indx, 0)\n",
    "    binned_score_subtract = do_time_bin(score_subtract, indx, 0)\n",
    "    \n",
    "    print(score_subtract.shape)\n",
    "    score_subtract=score_subtract[:, np.newaxis, np.newaxis] # [:,:, np.newaxis] when added more subjects\n",
    "    \n",
    "    print(score_subtract.shape)\n",
    "    t_obs, clusters, clusters_pv, H0 = mne.stats.spatio_temporal_cluster_1samp_test(score_subtract, tail=0)\n",
    "    \n",
    "    clust_pck = [t_obs, clusters, clusters_pv, H0]\n",
    "    \n",
    "    return binned_score_diag, clust_pck"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stat analysis for stratified shuffle split result (Alex suggestion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#binned_score_diag_sss, clust_pck_sss = stat_anal(scores_sss)\n",
    "#plot_scores_stat(binned_score_diag_sss, clust_pck_sss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Select some subjects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 35, 36, 43, 60, 71, 15, 21, 48]\n"
     ]
    }
   ],
   "source": [
    "#removed subj 25\n",
    "\n",
    "subj_indx = [ 1,  2,  3,  4,  5,  7,  8,  9, 10, 12, 15, 16, 17, 18, 19, 20, 21,\n",
    "       22, 23, 24, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38,\n",
    "       39, 42, 43, 44, 45, 46, 47, 48, 51, 52, 53, 55, 56, 57, 58, 59,\n",
    "       60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74]\n",
    "\n",
    "selected_subj = [subj_indx[0], subj_indx[29], subj_indx[30], subj_indx[35],\\\n",
    "                 subj_indx[49], subj_indx[60], 15, 21, 48]\n",
    "\n",
    "print(selected_subj) #[1, 34, 35, 42, 59, 70, 15, 21, 48]\n",
    "\n",
    "# selected_subj= [1, 34]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = StratifiedShuffleSplit(n_splits=args.n_splits, random_state=args.random_state)                                                    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating random data - null dist\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading ../../data/version5.2/preprocessed/epochs/epochs_sec_applyBaseline_subj1-afterRejICA-epo.fif ...\n",
      "Isotrak not found\n",
      "    Read a total of 1 projection items:\n",
      "        Average EEG reference (1 x 129) active\n",
      "    Found the data of interest:\n",
      "        t =    -400.00 ...    5000.00 ms\n",
      "        0 CTF compensation matrices available\n",
      "1197 matching events found\n",
      "Applying baseline correction (mode: mean)\n",
      "Adding metadata with 16 columns\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "Shape of data is\n",
      " :\n",
      "(352, 129, 113)\n",
      "the pattern for this subj is :=====================================\n",
      "1\n",
      "          \n",
      "===================================================================\n",
      "[0 1]\n",
      "[2 3]\n",
      "[........................................] 100.00% Fitting GeneralizingEstimato\n",
      "[........................................] 100.00% Scoring GeneralizingEstimato\n",
      "[........................................] 100.00% Fitting GeneralizingEstimato\n",
      "[........................................] 100.00% Scoring GeneralizingEstimato\n",
      "[........................................] 100.00% Fitting GeneralizingEstimato\n",
      "[........................................] 100.00% Scoring GeneralizingEstimato\n",
      "[........................................] 100.00% Fitting GeneralizingEstimato\n",
      "[........................................] 100.00% Scoring GeneralizingEstimato\n",
      "[........................................] 100.00% Fitting GeneralizingEstimato\n",
      "[........................................] 100.00% Scoring GeneralizingEstimato\n",
      "[........................................] 100.00% Fitting GeneralizingEstimato\n",
      "[........................................] 100.00% Scoring GeneralizingEstimato\n",
      "[0 1]\n",
      "[2 3]\n",
      "[........................................] 100.00% Fitting GeneralizingEstimato\n",
      "[........................................] 100.00% Scoring GeneralizingEstimato\n",
      "[........................................] 100.00% Fitting GeneralizingEstimato\n",
      "[........................................] 100.00% Scoring GeneralizingEstimato\n",
      "[........................................] 100.00% Fitting GeneralizingEstimato\n",
      "[........................................] 100.00% Scoring GeneralizingEstimato\n",
      "[........................................] 100.00% Fitting GeneralizingEstimato\n",
      "[........................................] 100.00% Scoring GeneralizingEstimato\n",
      "[........................................] 100.00% Fitting GeneralizingEstimato\n",
      "[........................................] 100.00% Scoring GeneralizingEstimato\n",
      "[........................................] 100.00% Fitting GeneralizingEstimato\n",
      "[........................................] 100.00% Scoring GeneralizingEstimato\n",
      "[0 1]\n",
      "[2 3]\n",
      "[........................................] 100.00% Fitting GeneralizingEstimato\n",
      "[........................................] 100.00% Scoring GeneralizingEstimato\n",
      "[........................................] 100.00% Fitting GeneralizingEstimato\n",
      "[........................................] 100.00% Scoring GeneralizingEstimato\n",
      "[........................................] 100.00% Fitting GeneralizingEstimato\n",
      "[........................................] 100.00% Scoring GeneralizingEstimato\n",
      "[........................................] 100.00% Fitting GeneralizingEstimato\n",
      "[........................................] 100.00% Scoring GeneralizingEstimato\n",
      "[........................................] 100.00% Fitting GeneralizingEstimato\n",
      "[........................................] 100.00% Scoring GeneralizingEstimato\n",
      "[........................................] 100.00% Fitting GeneralizingEstimato\n",
      "[........................................] 100.00% Scoring GeneralizingEstimato\n",
      "[0 1]\n",
      "[2 3]\n",
      "[........................................] 100.00% Fitting GeneralizingEstimato\n",
      "[........................................] 100.00% Scoring GeneralizingEstimato\n",
      "[........................................] 100.00% Fitting GeneralizingEstimato\n",
      "[........................................] 100.00% Scoring GeneralizingEstimato\n",
      "[........................................] 100.00% Fitting GeneralizingEstimato\n",
      "[........................................] 100.00% Scoring GeneralizingEstimato\n",
      "[........................................] 100.00% Fitting GeneralizingEstimato\n",
      "[........................................] 100.00% Scoring GeneralizingEstimato\n",
      "[........................................] 100.00% Fitting GeneralizingEstimato\n",
      "[........................................] 100.00% Scoring GeneralizingEstimato\n",
      "[........................................] 100.00% Fitting GeneralizingEstimato\n",
      "[........................................] 100.00% Scoring GeneralizingEstimato\n",
      "[0 1]\n",
      "[2 3]\n",
      "[........................................] 100.00% Fitting GeneralizingEstimato\n",
      "[........................................] 100.00% Scoring GeneralizingEstimato\n",
      "[........................................] 100.00% Fitting GeneralizingEstimato\n",
      "[........................................] 100.00% Scoring GeneralizingEstimato\n",
      "[........................................] 100.00% Fitting GeneralizingEstimato\n",
      "[........................................] 100.00% Scoring GeneralizingEstimato\n",
      "[........................................] 100.00% Fitting GeneralizingEstimato\n",
      "[........................................] 100.00% Scoring GeneralizingEstimato\n",
      "[........................................] 100.00% Fitting GeneralizingEstimato\n",
      "[........................................] 100.00% Scoring GeneralizingEstimato\n",
      "[........................................] 100.00% Fitting GeneralizingEstimato\n",
      "[........................................] 100.00% Scoring GeneralizingEstimato\n",
      "[0 1]\n",
      "[3 4]\n",
      "[........................................] 100.00% Fitting GeneralizingEstimato\n",
      "[........................................] 100.00% Scoring GeneralizingEstimato\n",
      "[........................................] 100.00% Fitting GeneralizingEstimato\n",
      "[........................................] 100.00% Scoring GeneralizingEstimato\n",
      "[........................................] 100.00% Fitting GeneralizingEstimato\n",
      "[........................................] 100.00% Scoring GeneralizingEstimato\n",
      "[........................................] 100.00% Fitting GeneralizingEstimato\n",
      "[........................................] 100.00% Scoring GeneralizingEstimato\n",
      "[........................................] 100.00% Fitting GeneralizingEstimato\n",
      "[........................................] 100.00% Scoring GeneralizingEstimato\n",
      "[........................................] 100.00% Fitting GeneralizingEstimato\n",
      "[........................................] 100.00% Scoring GeneralizingEstimato\n",
      "[0 1]\n",
      "[3 4]\n",
      "[........................................] 100.00% Fitting GeneralizingEstimato\n",
      "[........................................] 100.00% Scoring GeneralizingEstimato\n",
      "[........................................] 100.00% Fitting GeneralizingEstimato\n",
      "[........................................] 100.00% Scoring GeneralizingEstimato\n",
      "[........................................] 100.00% Fitting GeneralizingEstimato\n",
      "[........................................] 100.00% Scoring GeneralizingEstimato\n",
      "[........................................] 100.00% Fitting GeneralizingEstimato\n",
      "[........................................] 100.00% Scoring GeneralizingEstimato\n",
      "[........................................] 100.00% Fitting GeneralizingEstimato\n",
      "[........................................] 100.00% Scoring GeneralizingEstimato\n",
      "[........................................] 100.00% Fitting GeneralizingEstimato\n",
      "[........................................] 100.00% Scoring GeneralizingEstimato\n",
      "[0 1]\n",
      "[3 4]\n",
      "[........................................] 100.00% Fitting GeneralizingEstimato\n",
      "[........................................] 100.00% Scoring GeneralizingEstimato\n",
      "[........................................] 100.00% Fitting GeneralizingEstimato\n",
      "[........................................] 100.00% Scoring GeneralizingEstimato\n",
      "[........................................] 100.00% Fitting GeneralizingEstimato\n",
      "[........................................] 100.00% Scoring GeneralizingEstimato\n",
      "[........................................] 100.00% Fitting GeneralizingEstimato\n",
      "[........................................] 100.00% Scoring GeneralizingEstimato\n",
      "[........................................] 100.00% Fitting GeneralizingEstimato\n",
      "[........................................] 100.00% Scoring GeneralizingEstimato\n",
      "[........................................] 100.00% Fitting GeneralizingEstimato\n",
      "[........................................] 100.00% Scoring GeneralizingEstimato\n",
      "[0 1]\n",
      "[3 4]\n",
      "[........................................] 100.00% Fitting GeneralizingEstimato\n",
      "[........................................] 100.00% Scoring GeneralizingEstimato\n",
      "[........................................] 100.00% Fitting GeneralizingEstimato\n",
      "[........................................] 100.00% Scoring GeneralizingEstimato\n",
      "[........................................] 100.00% Fitting GeneralizingEstimato\n",
      "[........................................] 100.00% Scoring GeneralizingEstimato\n",
      "[........................................] 100.00% Fitting GeneralizingEstimato\n",
      "[........................................] 100.00% Scoring GeneralizingEstimato\n",
      "[........................................] 100.00% Fitting GeneralizingEstimato\n",
      "[........................................] 100.00% Scoring GeneralizingEstimato\n",
      "[........................................] 100.00% Fitting GeneralizingEstimato\n",
      "[........................................] 100.00% Scoring GeneralizingEstimato\n",
      "[0 1]\n",
      "[3 4]\n",
      "[........................................] 100.00% Fitting GeneralizingEstimato\n",
      "[........................................] 100.00% Scoring GeneralizingEstimato\n",
      "[........................................] 100.00% Fitting GeneralizingEstimato\n",
      "[........................................] 100.00% Scoring GeneralizingEstimato\n",
      "[........................................] 100.00% Fitting GeneralizingEstimato\n",
      "[........................................] 100.00% Scoring GeneralizingEstimato\n",
      "[........................................] 100.00% Fitting GeneralizingEstimato\n",
      "[........................................] 100.00% Scoring GeneralizingEstimato\n",
      "[........................................] 100.00% Fitting GeneralizingEstimato\n",
      "[........................................] 100.00% Scoring GeneralizingEstimato\n",
      "[........................................] 100.00% Fitting GeneralizingEstimato\n",
      "[........................................] 100.00% Scoring GeneralizingEstimato\n",
      "[0 1]\n",
      "[1 4]\n",
      "[........................................] 100.00% Fitting GeneralizingEstimato\n",
      "[........................................] 100.00% Scoring GeneralizingEstimato\n",
      "[........................................] 100.00% Fitting GeneralizingEstimato\n",
      "[........................................] 100.00% Scoring GeneralizingEstimato\n",
      "[........................................] 100.00% Fitting GeneralizingEstimato\n",
      "[........................................] 100.00% Scoring GeneralizingEstimato\n",
      "[........................................] 100.00% Fitting GeneralizingEstimato\n",
      "[........................................] 100.00% Scoring GeneralizingEstimato\n",
      "[........................................] 100.00% Fitting GeneralizingEstimato\n",
      "[........................................] 100.00% Scoring GeneralizingEstimato\n",
      "[........................................] 100.00% Fitting GeneralizingEstimato\n",
      "[........................................] 100.00% Scoring GeneralizingEstimato\n",
      "[0 1]\n",
      "[1 4]\n",
      "[........................................] 100.00% Fitting GeneralizingEstimato\n",
      "[........................................] 100.00% Scoring GeneralizingEstimato\n",
      "[........................................] 100.00% Fitting GeneralizingEstimato\n",
      "[........................................] 100.00% Scoring GeneralizingEstimato\n",
      "[........................................] 100.00% Fitting GeneralizingEstimato\n",
      "[........................................] 100.00% Scoring GeneralizingEstimato\n",
      "[........................................] 100.00% Fitting GeneralizingEstimato\n",
      "[........................................] 100.00% Scoring GeneralizingEstimato\n",
      "[........................................] 100.00% Fitting GeneralizingEstimato\n",
      "[........................................] 100.00% Scoring GeneralizingEstimato\n",
      "[........................................] 100.00% Fitting GeneralizingEstimato\n",
      "[........................................] 100.00% Scoring GeneralizingEstimato\n",
      "[0 1]\n",
      "[1 4]\n",
      "[........................................] 100.00% Fitting GeneralizingEstimato\n",
      "[........................................] 100.00% Scoring GeneralizingEstimato\n",
      "[........................................] 100.00% Fitting GeneralizingEstimato\n",
      "[........................................] 100.00% Scoring GeneralizingEstimato\n",
      "[........................................] 100.00% Fitting GeneralizingEstimato\n",
      "[........................................] 100.00% Scoring GeneralizingEstimato\n",
      "[........................................] 100.00% Fitting GeneralizingEstimato\n",
      "[........................................] 100.00% Scoring GeneralizingEstimato\n",
      "[........................................] 100.00% Fitting GeneralizingEstimato\n",
      "[........................................] 100.00% Scoring GeneralizingEstimato\n",
      "[........................................] 100.00% Fitting GeneralizingEstimato\n",
      "[........................................] 100.00% Scoring GeneralizingEstimato\n",
      "[0 1]\n",
      "[1 4]\n",
      "[........................................] 100.00% Fitting GeneralizingEstimato\n",
      "[........................................] 100.00% Scoring GeneralizingEstimato\n",
      "[........................................] 100.00% Fitting GeneralizingEstimato\n",
      "[........................................] 100.00% Scoring GeneralizingEstimato\n",
      "[........................................] 100.00% Fitting GeneralizingEstimato\n",
      "[........................................] 100.00% Scoring GeneralizingEstimato\n",
      "[........................................] 100.00% Fitting GeneralizingEstimato\n",
      "[........................................] 100.00% Scoring GeneralizingEstimato\n",
      "[........................................] 100.00% Fitting GeneralizingEstimato\n",
      "[........................................] 100.00% Scoring GeneralizingEstimato\n",
      "[........................................] 100.00% Fitting GeneralizingEstimato\n",
      "[........................................] 100.00% Scoring GeneralizingEstimato\n",
      "[0 1]\n",
      "[1 4]\n",
      "[........................................] 100.00% Fitting GeneralizingEstimato\n",
      "[........................................] 100.00% Scoring GeneralizingEstimato\n",
      "[........................................] 100.00% Fitting GeneralizingEstimato\n",
      "[........................................] 100.00% Scoring GeneralizingEstimato\n",
      "[........................................] 100.00% Fitting GeneralizingEstimato\n",
      "[........................................] 100.00% Scoring GeneralizingEstimato\n",
      "[........................................] 100.00% Fitting GeneralizingEstimato\n",
      "[........................................] 100.00% Scoring GeneralizingEstimato\n",
      "[........................................] 100.00% Fitting GeneralizingEstimato\n",
      "[........................................] 100.00% Scoring GeneralizingEstimato\n",
      "[........................................] 100.00% Fitting GeneralizingEstimato\n",
      "[........................................] 100.00% Scoring GeneralizingEstimato\n",
      "[0 1]\n",
      "[1 2]\n",
      "[........................................] 100.00% Fitting GeneralizingEstimato\n",
      "[........................................] 100.00% Scoring GeneralizingEstimato\n",
      "[........................................] 100.00% Fitting GeneralizingEstimato\n",
      "[........................................] 100.00% Scoring GeneralizingEstimato\n",
      "[........................................] 100.00% Fitting GeneralizingEstimato\n",
      "[........................................] 100.00% Scoring GeneralizingEstimato\n",
      "[........................................] 100.00% Fitting GeneralizingEstimato\n",
      "[........................................] 100.00% Scoring GeneralizingEstimato\n",
      "[........................................] 100.00% Fitting GeneralizingEstimato\n",
      "[........................................] 100.00% Scoring GeneralizingEstimato\n",
      "[........................................] 100.00% Fitting GeneralizingEstimato\n",
      "[........................................] 100.00% Scoring GeneralizingEstimato\n",
      "[0 1]\n",
      "[1 2]\n",
      "[........................................] 100.00% Fitting GeneralizingEstimato\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[........................................] 100.00% Scoring GeneralizingEstimato\n",
      "[........................................] 100.00% Fitting GeneralizingEstimato\n",
      "[........................................] 100.00% Scoring GeneralizingEstimato\n",
      "[........................................] 100.00% Fitting GeneralizingEstimato\n",
      "[........................................] 100.00% Scoring GeneralizingEstimato\n",
      "[........................................] 100.00% Fitting GeneralizingEstimato\n",
      "[........................................] 100.00% Scoring GeneralizingEstimato\n",
      "[........................................] 100.00% Fitting GeneralizingEstimato\n",
      "[........................................] 100.00% Scoring GeneralizingEstimato\n",
      "[........................................] 100.00% Fitting GeneralizingEstimato\n",
      "[........................................] 100.00% Scoring GeneralizingEstimato\n",
      "[0 1]\n",
      "[1 2]\n",
      "[........................................] 100.00% Fitting GeneralizingEstimato\n",
      "[........................................] 100.00% Scoring GeneralizingEstimato\n",
      "[........................................] 100.00% Fitting GeneralizingEstimato\n",
      "[........................................] 100.00% Scoring GeneralizingEstimato\n",
      "[........................................] 100.00% Fitting GeneralizingEstimato\n",
      "[........................................] 100.00% Scoring GeneralizingEstimato\n",
      "[........................................] 100.00% Fitting GeneralizingEstimato\n",
      "[........................................] 100.00% Scoring GeneralizingEstimato\n",
      "[........................................] 100.00% Fitting GeneralizingEstimato\n",
      "[........................................] 100.00% Scoring GeneralizingEstimato\n",
      "[........................................] 100.00% Fitting GeneralizingEstimato\n",
      "[........................................] 100.00% Scoring GeneralizingEstimato\n",
      "[0 1]\n",
      "[1 2]\n",
      "[........................................] 100.00% Fitting GeneralizingEstimato\n",
      "[........................................] 100.00% Scoring GeneralizingEstimato\n",
      "[........................................] 100.00% Fitting GeneralizingEstimato\n",
      "[........................................] 100.00% Scoring GeneralizingEstimato\n",
      "[........................................] 100.00% Fitting GeneralizingEstimato\n",
      "[........................................] 100.00% Scoring GeneralizingEstimato\n",
      "[........................................] 100.00% Fitting GeneralizingEstimato\n",
      "[........................................] 100.00% Scoring GeneralizingEstimato\n",
      "[........................................] 100.00% Fitting GeneralizingEstimato\n",
      "[........................................] 100.00% Scoring GeneralizingEstimato\n",
      "[........................................] 100.00% Fitting GeneralizingEstimato\n",
      "[........................................] 100.00% Scoring GeneralizingEstimato\n",
      "[0 1]\n",
      "[1 2]\n",
      "[........................................] 100.00% Fitting GeneralizingEstimato\n",
      "[........................................] 100.00% Scoring GeneralizingEstimato\n",
      "[........................................] 100.00% Fitting GeneralizingEstimato\n",
      "[........................................] 100.00% Scoring GeneralizingEstimato\n",
      "[........................................] 100.00% Fitting GeneralizingEstimato\n",
      "[........................................] 100.00% Scoring GeneralizingEstimato\n",
      "[........................................] 100.00% Fitting GeneralizingEstimato\n",
      "[........................................] 100.00% Scoring GeneralizingEstimato\n",
      "[........................................] 100.00% Fitting GeneralizingEstimato\n",
      "[........................................] 100.00% Scoring GeneralizingEstimato\n",
      "[........................................] 100.00% Fitting GeneralizingEstimato\n",
      "[........................................] 100.00% Scoring GeneralizingEstimato\n",
      "Reading ../../data/version5.2/preprocessed/epochs/epochs_sec_applyBaseline_subj35-afterRejICA-epo.fif ...\n",
      "Isotrak not found\n",
      "    Read a total of 1 projection items:\n",
      "        Average EEG reference (1 x 129) active\n",
      "    Found the data of interest:\n",
      "        t =    -400.00 ...    5000.00 ms\n",
      "        0 CTF compensation matrices available\n",
      "1174 matching events found\n",
      "Applying baseline correction (mode: mean)\n",
      "Adding metadata with 16 columns\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "Shape of data is\n",
      " :\n",
      "(345, 129, 113)\n",
      "the pattern for this subj is :=====================================\n",
      "2\n",
      "          \n",
      "===================================================================\n",
      "[0 1]\n",
      "[3 4]\n",
      "[........................................] 100.00% Fitting GeneralizingEstimato\n",
      "[........................................] 100.00% Scoring GeneralizingEstimato\n",
      "[........................................] 100.00% Fitting GeneralizingEstimato\n",
      "[........................................] 100.00% Scoring GeneralizingEstimato\n",
      "[........................................] 100.00% Fitting GeneralizingEstimato\n",
      "[........................................] 100.00% Scoring GeneralizingEstimato\n",
      "[........................................] 100.00% Fitting GeneralizingEstimato\n",
      "[........................................] 100.00% Scoring GeneralizingEstimato\n",
      "[........................................] 100.00% Fitting GeneralizingEstimato\n",
      "[........................................] 100.00% Scoring GeneralizingEstimato\n",
      "[........................................] 100.00% Fitting GeneralizingEstimato\n",
      "[........................................] 100.00% Scoring GeneralizingEstimato\n",
      "[0 1]\n",
      "[3 4]\n",
      "[........................................] 100.00% Fitting GeneralizingEstimato\n",
      "[........................................] 100.00% Scoring GeneralizingEstimato\n",
      "[........................................] 100.00% Fitting GeneralizingEstimato\n",
      "[........................................] 100.00% Scoring GeneralizingEstimato\n",
      "[........................................] 100.00% Fitting GeneralizingEstimato\n",
      "[........................................] 100.00% Scoring GeneralizingEstimato\n",
      "[........................................] 100.00% Fitting GeneralizingEstimato\n",
      "[........................................] 100.00% Scoring GeneralizingEstimato\n",
      "[........................................] 100.00% Fitting GeneralizingEstimato\n",
      "[........................................] 100.00% Scoring GeneralizingEstimato\n",
      "[........................................] 100.00% Fitting GeneralizingEstimato\n",
      "[........................................] 100.00% Scoring GeneralizingEstimato\n",
      "[0 1]\n",
      "[3 4]\n",
      "[........................................] 100.00% Fitting GeneralizingEstimato\n",
      "[........................................] 100.00% Scoring GeneralizingEstimato\n",
      "[........................................] 100.00% Fitting GeneralizingEstimato\n",
      "[........................................] 100.00% Scoring GeneralizingEstimato\n",
      "[........................................] 100.00% Fitting GeneralizingEstimato\n",
      "[........................................] 100.00% Scoring GeneralizingEstimato\n",
      "[........................................] 100.00% Fitting GeneralizingEstimato\n",
      "[........................................] 100.00% Scoring GeneralizingEstimato\n",
      "[........................................] 100.00% Fitting GeneralizingEstimato\n",
      "[........................................] 100.00% Scoring GeneralizingEstimato\n",
      "[........................................] 100.00% Fitting GeneralizingEstimato\n",
      "[........................................] 100.00% Scoring GeneralizingEstimato\n",
      "[0 1]\n",
      "[3 4]\n",
      "[........................................] 100.00% Fitting GeneralizingEstimato\n",
      "[........................................] 100.00% Scoring GeneralizingEstimato\n",
      "[........................................] 100.00% Fitting GeneralizingEstimato\n",
      "[........................................] 100.00% Scoring GeneralizingEstimato\n",
      "[........................................] 100.00% Fitting GeneralizingEstimato\n",
      "[........................................] 100.00% Scoring GeneralizingEstimato\n",
      "[........................................] 100.00% Fitting GeneralizingEstimato\n",
      "[........................................] 100.00% Scoring GeneralizingEstimato\n",
      "[........................................] 100.00% Fitting GeneralizingEstimato\n",
      "[........................................] 100.00% Scoring GeneralizingEstimato\n",
      "[........................................] 100.00% Fitting GeneralizingEstimato\n",
      "[........................................] 100.00% Scoring GeneralizingEstimato\n",
      "[0 1]\n",
      "[3 4]\n",
      "[........................................] 100.00% Fitting GeneralizingEstimato\n",
      "[........................................] 100.00% Scoring GeneralizingEstimato\n",
      "[........................................] 100.00% Fitting GeneralizingEstimato\n",
      "[........................................] 100.00% Scoring GeneralizingEstimato\n",
      "[........................................] 100.00% Fitting GeneralizingEstimato\n",
      "[........................................] 100.00% Scoring GeneralizingEstimato\n",
      "[........................................] 100.00% Fitting GeneralizingEstimato\n",
      "[........................................] 100.00% Scoring GeneralizingEstimato\n",
      "[........................................] 100.00% Fitting GeneralizingEstimato\n",
      "[........................................] 100.00% Scoring GeneralizingEstimato\n",
      "[........................................] 100.00% Fitting GeneralizingEstimato\n",
      "[........................................] 100.00% Scoring GeneralizingEstimato\n",
      "[0 1]\n",
      "[1 4]\n",
      "[........................................] 100.00% Fitting GeneralizingEstimato\n",
      "[........................................] 100.00% Scoring GeneralizingEstimato\n",
      "[........................................] 100.00% Fitting GeneralizingEstimato\n",
      "[........................................] 100.00% Scoring GeneralizingEstimato\n",
      "[........................................] 100.00% Fitting GeneralizingEstimato\n",
      "[........................................] 100.00% Scoring GeneralizingEstimato\n",
      "[........................................] 100.00% Fitting GeneralizingEstimato\n",
      "[........................................] 100.00% Scoring GeneralizingEstimato\n",
      "[........................................] 100.00% Fitting GeneralizingEstimato\n",
      "[........................................] 100.00% Scoring GeneralizingEstimato\n",
      "[........................................] 100.00% Fitting GeneralizingEstimato\n",
      "[........................................] 100.00% Scoring GeneralizingEstimato\n",
      "[0 1]\n",
      "[1 4]\n",
      "[........................................] 100.00% Fitting GeneralizingEstimato\n",
      "[........................................] 100.00% Scoring GeneralizingEstimato\n",
      "[........................................] 100.00% Fitting GeneralizingEstimato\n",
      "[........................................] 100.00% Scoring GeneralizingEstimato\n",
      "[........................................] 100.00% Fitting GeneralizingEstimato\n",
      "[........................................] 100.00% Scoring GeneralizingEstimato\n",
      "[........................................] 100.00% Fitting GeneralizingEstimato\n",
      "[........................................] 100.00% Scoring GeneralizingEstimato\n",
      "[........................................] 100.00% Fitting GeneralizingEstimato\n",
      "[........................................] 100.00% Scoring GeneralizingEstimato\n",
      "[........................................] 100.00% Fitting GeneralizingEstimato\n",
      "[........................................] 100.00% Scoring GeneralizingEstimato\n",
      "[0 1]\n",
      "[1 4]\n",
      "[........................................] 100.00% Fitting GeneralizingEstimato\n",
      "[........................................] 100.00% Scoring GeneralizingEstimato\n",
      "[........................................] 100.00% Fitting GeneralizingEstimato\n",
      "[........................................] 100.00% Scoring GeneralizingEstimato\n",
      "[........................................] 100.00% Fitting GeneralizingEstimato\n",
      "[........................................] 100.00% Scoring GeneralizingEstimato\n",
      "[........................................] 100.00% Fitting GeneralizingEstimato\n",
      "[........................................] 100.00% Scoring GeneralizingEstimato\n",
      "[........................................] 100.00% Fitting GeneralizingEstimato\n",
      "[........................................] 100.00% Scoring GeneralizingEstimato\n",
      "[........................................] 100.00% Fitting GeneralizingEstimato\n",
      "[........................................] 100.00% Scoring GeneralizingEstimato\n",
      "[0 1]\n",
      "[1 4]\n",
      "[........................................] 100.00% Fitting GeneralizingEstimato\n",
      "[........................................] 100.00% Scoring GeneralizingEstimato\n",
      "[........................................] 100.00% Fitting GeneralizingEstimato\n",
      "[........................................] 100.00% Scoring GeneralizingEstimato\n",
      "[........................................] 100.00% Fitting GeneralizingEstimato\n",
      "[........................................] 100.00% Scoring GeneralizingEstimato\n",
      "[........................................] 100.00% Fitting GeneralizingEstimato\n",
      "[........................................] 100.00% Scoring GeneralizingEstimato\n",
      "[........................................] 100.00% Fitting GeneralizingEstimato\n",
      "[........................................] 100.00% Scoring GeneralizingEstimato\n",
      "[........................................] 100.00% Fitting GeneralizingEstimato\n",
      "[........................................] 100.00% Scoring GeneralizingEstimato\n",
      "[0 1]\n",
      "[1 4]\n",
      "[........................................] 100.00% Fitting GeneralizingEstimato\n",
      "[........................................] 100.00% Scoring GeneralizingEstimato\n",
      "[........................................] 100.00% Fitting GeneralizingEstimato\n",
      "[........................................] 100.00% Scoring GeneralizingEstimato\n",
      "[........................................] 100.00% Fitting GeneralizingEstimato\n",
      "[........................................] 100.00% Scoring GeneralizingEstimato\n",
      "[........................................] 100.00% Fitting GeneralizingEstimato\n",
      "[........................................] 100.00% Scoring GeneralizingEstimato\n",
      "[........................................] 100.00% Fitting GeneralizingEstimato\n",
      "[........................................] 100.00% Scoring GeneralizingEstimato\n",
      "[........................................] 100.00% Fitting GeneralizingEstimato\n",
      "[........................................] 100.00% Scoring GeneralizingEstimato\n",
      "[0 1]\n",
      "[1 2]\n",
      "[........................................] 100.00% Fitting GeneralizingEstimato\n",
      "[........................................] 100.00% Scoring GeneralizingEstimato\n",
      "[........................................] 100.00% Fitting GeneralizingEstimato\n",
      "[........................................] 100.00% Scoring GeneralizingEstimato\n",
      "[........................................] 100.00% Fitting GeneralizingEstimato\n",
      "[........................................] 100.00% Scoring GeneralizingEstimato\n",
      "[........................................] 100.00% Fitting GeneralizingEstimato\n",
      "[........................................] 100.00% Scoring GeneralizingEstimato\n",
      "[........................................] 100.00% Fitting GeneralizingEstimato\n",
      "[........................................] 100.00% Scoring GeneralizingEstimato\n",
      "[........................................] 100.00% Fitting GeneralizingEstimato\n",
      "[........................................] 100.00% Scoring GeneralizingEstimato\n",
      "[0 1]\n",
      "[1 2]\n",
      "[........................................] 100.00% Fitting GeneralizingEstimato\n",
      "[........................................] 100.00% Scoring GeneralizingEstimato\n",
      "[........................................] 100.00% Fitting GeneralizingEstimato\n",
      "[........................................] 100.00% Scoring GeneralizingEstimato\n",
      "[........................................] 100.00% Fitting GeneralizingEstimato\n",
      "[........................................] 100.00% Scoring GeneralizingEstimato\n",
      "[........................................] 100.00% Fitting GeneralizingEstimato\n",
      "[........................................] 100.00% Scoring GeneralizingEstimato\n",
      "[........................................] 100.00% Fitting GeneralizingEstimato\n",
      "[........................................] 100.00% Scoring GeneralizingEstimato\n",
      "[........................................] 100.00% Fitting GeneralizingEstimato\n",
      "[........................................] 100.00% Scoring GeneralizingEstimato\n",
      "[0 1]\n",
      "[1 2]\n",
      "[........................................] 100.00% Fitting GeneralizingEstimato\n",
      "[........................................] 100.00% Scoring GeneralizingEstimato\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[........................................] 100.00% Fitting GeneralizingEstimato\n",
      "[........................................] 100.00% Scoring GeneralizingEstimato\n",
      "[........................................] 100.00% Fitting GeneralizingEstimato\n",
      "[........................................] 100.00% Scoring GeneralizingEstimato\n",
      "[........................................] 100.00% Fitting GeneralizingEstimato\n",
      "[........................................] 100.00% Scoring GeneralizingEstimato\n",
      "[........................................] 100.00% Fitting GeneralizingEstimato\n",
      "[........................................] 100.00% Scoring GeneralizingEstimato\n",
      "[........................................] 100.00% Fitting GeneralizingEstimato\n",
      "[........................................] 100.00% Scoring GeneralizingEstimato\n",
      "[0 1]\n",
      "[1 2]\n",
      "[........................................] 100.00% Fitting GeneralizingEstimato\n",
      "[........................................] 100.00% Scoring GeneralizingEstimato\n",
      "[........................................] 100.00% Fitting GeneralizingEstimato\n",
      "[........................................] 100.00% Scoring GeneralizingEstimato\n",
      "[........................................] 100.00% Fitting GeneralizingEstimato\n",
      "[........................................] 100.00% Scoring GeneralizingEstimato\n",
      "[........................................] 100.00% Fitting GeneralizingEstimato\n",
      "[........................................] 100.00% Scoring GeneralizingEstimato\n",
      "[........................................] 100.00% Fitting GeneralizingEstimato\n",
      "[........................................] 100.00% Scoring GeneralizingEstimato\n",
      "[........................................] 100.00% Fitting GeneralizingEstimato\n",
      "[........................................] 100.00% Scoring GeneralizingEstimato\n",
      "[0 1]\n",
      "[1 2]\n",
      "[........................................] 100.00% Fitting GeneralizingEstimato\n",
      "[........................................] 100.00% Scoring GeneralizingEstimato\n",
      "[........................................] 100.00% Fitting GeneralizingEstimato\n",
      "[........................................] 100.00% Scoring GeneralizingEstimato\n",
      "[........................................] 100.00% Fitting GeneralizingEstimato\n",
      "[........................................] 100.00% Scoring GeneralizingEstimato\n",
      "[........................................] 100.00% Fitting GeneralizingEstimato\n",
      "[........................................] 100.00% Scoring GeneralizingEstimato\n",
      "[........................................] 100.00% Fitting GeneralizingEstimato\n",
      "[........................................] 100.00% Scoring GeneralizingEstimato\n",
      "[........................................] 100.00% Fitting GeneralizingEstimato\n",
      "[........................................] 100.00% Scoring GeneralizingEstimato\n",
      "[0 1]\n",
      "[2 3]\n",
      "[........................................] 100.00% Fitting GeneralizingEstimato\n",
      "[........................................] 100.00% Scoring GeneralizingEstimato\n",
      "[........................................] 100.00% Fitting GeneralizingEstimato\n",
      "[........................................] 100.00% Scoring GeneralizingEstimato\n",
      "[........................................] 100.00% Fitting GeneralizingEstimato\n",
      "[........................................] 100.00% Scoring GeneralizingEstimato\n",
      "[........................................] 100.00% Fitting GeneralizingEstimato\n",
      "[........................................] 100.00% Scoring GeneralizingEstimato\n",
      "[........................................] 100.00% Fitting GeneralizingEstimato\n",
      "[........................................] 100.00% Scoring GeneralizingEstimato\n",
      "[........................................] 100.00% Fitting GeneralizingEstimato\n",
      "[........................................] 100.00% Scoring GeneralizingEstimato\n",
      "[0 1]\n",
      "[2 3]\n",
      "[........................................] 100.00% Fitting GeneralizingEstimato\n",
      "[........................................] 100.00% Scoring GeneralizingEstimato\n",
      "[........................................] 100.00% Fitting GeneralizingEstimato\n",
      "[........................................] 100.00% Scoring GeneralizingEstimato\n",
      "[........................................] 100.00% Fitting GeneralizingEstimato\n",
      "[........................................] 100.00% Scoring GeneralizingEstimato\n",
      "[........................................] 100.00% Fitting GeneralizingEstimato\n",
      "[........................................] 100.00% Scoring GeneralizingEstimato\n",
      "[........................................] 100.00% Fitting GeneralizingEstimato\n",
      "[........................................] 100.00% Scoring GeneralizingEstimato\n",
      "[........................................] 100.00% Fitting GeneralizingEstimato\n",
      "[........................................] 100.00% Scoring GeneralizingEstimato\n",
      "[0 1]\n",
      "[2 3]\n",
      "[........................................] 100.00% Fitting GeneralizingEstimato\n",
      "[........................................] 100.00% Scoring GeneralizingEstimato\n",
      "[........................................] 100.00% Fitting GeneralizingEstimato\n",
      "[........................................] 100.00% Scoring GeneralizingEstimato\n",
      "[........................................] 100.00% Fitting GeneralizingEstimato\n",
      "[........................................] 100.00% Scoring GeneralizingEstimato\n",
      "[........................................] 100.00% Fitting GeneralizingEstimato\n",
      "[................                        ]  40.09% Scoring GeneralizingEstimato"
     ]
    }
   ],
   "source": [
    "rand_avgp1_score=[]\n",
    "rand_avgp2_score=[]\n",
    "\n",
    "rand_avgp1_diag=[]\n",
    "rand_avgp2_diag=[]\n",
    "\n",
    "\n",
    "for subj_id in selected_subj:\n",
    "    args.subj_num = subj_id\n",
    "    Grp1, Grp2, Grp3, Grp4, main_ptrn = read_prep_epochs(args)\n",
    "\n",
    "    sc_pck_G1, sc_pck_fit_G1 = generate_rand(args, Grp1, cv)\n",
    "    sc_pck_G2, sc_pck_fit_G2 = generate_rand(args, Grp2, cv)\n",
    "    sc_pck_G3, sc_pck_fit_G3 = generate_rand(args, Grp3, cv)\n",
    "    sc_pck_G4, sc_pck_fit_G4 = generate_rand(args, Grp4, cv)\n",
    "    \n",
    "    # unpack them\n",
    "    score_G1, score_diag_G1 = sc_pck_G1\n",
    "    score_G2, score_diag_G2 = sc_pck_G2\n",
    "    score_G3, score_diag_G3 = sc_pck_G3\n",
    "    score_G4, score_diag_G4 = sc_pck_G4\n",
    "    \n",
    "    # integrate them for all subjects\n",
    "    if main_ptrn == 1:\n",
    "        rand_avgp1_score.append(sc_pck_G1)\n",
    "        rand_avgp1_diag.append(score_diag_G1)\n",
    "        \n",
    "        rand_avgp1_score.append(sc_pck_G2)\n",
    "        rand_avgp1_diag.append(score_diag_G2)\n",
    "        \n",
    "        rand_avgp1_score.append(sc_pck_G3)\n",
    "        rand_avgp1_diag.append(score_diag_G3)\n",
    "        \n",
    "        rand_avgp1_score.append(sc_pck_G4)\n",
    "        rand_avgp1_diag.append(score_diag_G4)\n",
    "    elif main_ptrn == 2:\n",
    "        rand_avgp2_score.append(sc_pck_G1)\n",
    "        rand_avgp2_diag.append(score_diag_G1)\n",
    "        \n",
    "        rand_avgp2_score.append(sc_pck_G2)\n",
    "        rand_avgp2_diag.append(score_diag_G2)\n",
    "        \n",
    "        rand_avgp2_score.append(sc_pck_G3)\n",
    "        rand_avgp2_diag.append(score_diag_G3)\n",
    "        \n",
    "        rand_avgp2_score.append(sc_pck_G4)\n",
    "        rand_avgp2_diag.append(score_diag_G4)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating decoding/tempGeneralization results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avgp1_score=[]\n",
    "avgp2_score=[]\n",
    "\n",
    "avgp1_diag=[]\n",
    "avgp2_diag=[]\n",
    "\n",
    "\n",
    "for subj_id in selected_subj:\n",
    "    args.subj_num = subj_id\n",
    "    Grp1, Grp2, Grp3, Grp4, main_ptrn = read_prep_epochs(args)\n",
    "    \n",
    "    sc_pck_G1, sc_pck_fit_G1 = apply_tempGen(args, Grp1, cv)\n",
    "    sc_pck_G2, sc_pck_fit_G2 = apply_tempGen(args, Grp2, cv)\n",
    "    sc_pck_G3, sc_pck_fit_G3 = apply_tempGen(args, Grp3, cv)\n",
    "    sc_pck_G4, sc_pck_fit_G4 = apply_tempGen(args, Grp4, cv)\n",
    "    \n",
    "    # unpack them\n",
    "    score_G1, score_diag_G1 = sc_pck_G1\n",
    "    score_G2, score_diag_G2 = sc_pck_G2\n",
    "    score_G3, score_diag_G3 = sc_pck_G3\n",
    "    score_G4, score_diag_G4 = sc_pck_G4\n",
    "    \n",
    "    # integrate them for all subjects\n",
    "    if main_ptrn == 1:\n",
    "        avgp1_score.append(sc_pck_G1)\n",
    "        avgp1_diag.append(score_diag_G1)\n",
    "        \n",
    "        avgp1_score.append(sc_pck_G2)\n",
    "        avgp1_diag.append(score_diag_G2)\n",
    "        \n",
    "        avgp1_score.append(sc_pck_G3)\n",
    "        avgp1_diag.append(score_diag_G3)\n",
    "        \n",
    "        avgp1_score.append(sc_pck_G4)\n",
    "        avgp1_diag.append(score_diag_G4)\n",
    "    elif main_ptrn == 2:\n",
    "        avgp2_score.append(sc_pck_G1)\n",
    "        avgp2_diag.append(score_diag_G1)\n",
    "        \n",
    "        avgp2_score.append(sc_pck_G2)\n",
    "        avgp2_diag.append(score_diag_G2)\n",
    "        \n",
    "        avgp2_score.append(sc_pck_G3)\n",
    "        avgp2_diag.append(score_diag_G3)\n",
    "        \n",
    "        avgp2_score.append(sc_pck_G4)\n",
    "        avgp2_diag.append(score_diag_G4)\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start the job at 2:11 am\n",
    "import pickle\n",
    "avgp2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fn_save = \n",
    "if main_ptrn == 1:\n",
    "#     subj_pck_p1 = [avgp1_score, avgp1_diag, avgp1_substract]\n",
    "#     subj_rand_pck_p1 = [avgp1_score, avgp1_diag, avgp1_substract]\n",
    "    subj_pck_p1 = [avgp1_score, avgp1_diag]\n",
    "    subj_rand_pck_p1 = [avgp1_score, avgp1_diag]\n",
    "    subj_scores_pck = [subj_pck_p1, subj_rand_pck_p1]\n",
    "    \n",
    "    with open('sc_subj_pck', 'wb') as f:\n",
    "        pickle.dump(sc_subj_pck, f)\n",
    "    \n",
    "elif main_ptrn == 2:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(113, 113)\n",
      "(113,)\n",
      "(113,)\n"
     ]
    }
   ],
   "source": [
    "print(avgp1_score[0][0].shape)\n",
    "print(avgp1_score[0][1].shape)\n",
    "print(avgp1_score[0][2].shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# General decoder params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "nSplits=5\n",
    "scoring='roc_auc'\n",
    "randState=0\n",
    "maxIter=10000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up a decoder\n",
    "\n",
    "\n",
    "## Using StandardScaler from sklearn.preprocessing:\n",
    "### mne.decoding.Scaler\n",
    "    scales each channel \n",
    "    using mean and standard deviation computed across all of its time points and epochs. \n",
    "###  sklearn.preprocessing.StandardScaler offered by scikit-learn\n",
    "    These scale each classification feature, e.g. each time point for each channel, \n",
    "    with mean and standard deviation computed across epochs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
