{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mne\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from mne.datasets import sample\n",
    "from mne.decoding import cross_val_multiscore, LinearModel, GeneralizingEstimator, Scaler, \\\n",
    "                         Vectorizer\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_score, StratifiedShuffleSplit, \\\n",
    "                                    RepeatedStratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "import argparse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class arguments():\n",
    "    SAVE_EPOCH_ROOT = '../../data/preprocessed/epochs/aft_ICA_rej/'\n",
    "    SAVE_RESULT_ROOT = '../../results/decoding/permtest/'\n",
    "    cond_filter ='none' # {none,non_symm}\n",
    "    cond_block ='early' #{early,later}\n",
    "    cond_time = 'prestim' #{prestim,poststim}\n",
    "    cond_decoding = 'none' #{none,removeevoked,resampled}\n",
    "    subj_num = 1\n",
    "    applyBaseline_bool = 0\n",
    "    pre_tmin = -0.4\n",
    "    pre_tmax = 0.05\n",
    "    post_tmin = 0.05\n",
    "    post_tmax = 0.45\n",
    "    num_classes = 2\n",
    "    normalization_type = 'normal'# {normal,lstmPaper}\n",
    "    gen_rand_perm = 0\n",
    "    null_max_iter = 10000\n",
    "    loop_null_iter = 5\n",
    "    gen_decoder_scores = 1\n",
    "    n_splits = 5\n",
    "    random_state = 42 \n",
    "    max_iter = 10000\n",
    "    n_jobs = 1\n",
    "    scoring = 'roc_auc'\n",
    "    \n",
    "args = arguments()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read each subj and preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Reading and preparing epoch data to create each 4 grous and 2 pattern\n",
    "\"\"\"\n",
    "def read_prep_epochs(args):\n",
    "    if args.applyBaseline_bool:\n",
    "        filename_epoch = args.SAVE_EPOCH_ROOT + \\\n",
    "                         'epochs_sec_applyBaseline_subj%s-afterRejICA-epo.fif' \\\n",
    "                          %args.subj_num\n",
    "    else:\n",
    "        filename_epoch = args.SAVE_EPOCH_ROOT + \\\n",
    "                         'epochs_sec_subj%s-afterRejICA-epo.fif' \\\n",
    "                         %args.subj_num\n",
    "    epochs_orig = mne.read_epochs(filename_epoch, proj=True, preload=True,\n",
    "                                  verbose=None)\n",
    "    epochs = epochs_orig.copy()\n",
    "    subset = epochs['pred']['non'].copy()\n",
    "    subset = subset.pick_types(eeg=True)\n",
    "    if (args.cond_decoding=='removeevoked'):\n",
    "        # REMOVE EVOKED RESP.\n",
    "        subset.subtract_evoked()    # remove evoked response\n",
    "    elif (args.cond_decoding=='resampled'):\n",
    "        # RESAMPLE\n",
    "        subset = subset.resample(args.n_resampling, npad='auto')\n",
    "    else:\n",
    "        pass\n",
    "    ##==========================================================================\n",
    "    if subset['Block==7'].metadata.Ptrn_Type.values.shape[0]>0:\n",
    "       main_ptrn = subset['Block==7'].metadata.Ptrn_Type.values[0]\n",
    "    else:\n",
    "       main_ptrn = subset['Block==8'].metadata.Ptrn_Type.values[0]\n",
    "    ##==========================================================================\n",
    "    if args.cond_block=='early': #block 3-6\n",
    "        subset = subset['Block<7'].copy()\n",
    "        subset = subset['Block>2'].copy()\n",
    "    elif args.cond_block=='later':#block 7-10\n",
    "        subset = subset['Block<11'].copy()\n",
    "        subset = subset['Block>6'].copy()\n",
    "    ##==========================================================================\n",
    "    if (args.cond_time=='prestim'):\n",
    "        subset= subset.crop(tmin=-0.4, tmax=0.05)\n",
    "    if (args.cond_time=='poststim'):\n",
    "        subset= subset.crop(tmin=0.05, tmax=0.45)\n",
    "    ##==========================================================================\n",
    "    # Group data based on the previous trial\n",
    "    Grp1 = subset['Trgt_Loc_prev==1'].copy()\n",
    "    Grp2 = subset['Trgt_Loc_prev==2'].copy()\n",
    "    Grp3 = subset['Trgt_Loc_prev==3'].copy()\n",
    "    Grp4 = subset['Trgt_Loc_prev==4'].copy()\n",
    "    if main_ptrn==1:\n",
    "        Grp1 = Grp1['Trgt_Loc_main!=4'].copy()\n",
    "        Grp2 = Grp2['Trgt_Loc_main!=1'].copy()\n",
    "        Grp3 = Grp3['Trgt_Loc_main!=2'].copy()\n",
    "        Grp4 = Grp4['Trgt_Loc_main!=3'].copy()\n",
    "    ##==========================================================================\n",
    "    frequencies = np.arange(3, 13, 2)\n",
    "    if args.cond_decoding=='non_symm':\n",
    "        Grp1 = apply_nonSymm_filter(Grp1, frequencies)\n",
    "        Grp2 = apply_nonSymm_filter(Grp2, frequencies)\n",
    "        Grp3 = apply_nonSymm_filter(Grp3, frequencies)\n",
    "        Grp4 = apply_nonSymm_filter(Grp4, frequencies)\n",
    "    ##==========================================================================\n",
    "    print('the pattern for this subj is :=====================================')\n",
    "    print(main_ptrn)\n",
    "    print('          ')\n",
    "    print('===================================================================')\n",
    "    ##==========================================================================\n",
    "    # Normalizing the data for each subject\n",
    "    if args.normalization_type=='normal':\n",
    "        Grp1._data = (Grp1._data - np.mean(Grp1._data)) / np.std(Grp1._data)\n",
    "        Grp2._data = (Grp2._data - np.mean(Grp2._data)) / np.std(Grp2._data)\n",
    "        Grp3._data = (Grp3._data - np.mean(Grp3._data)) / np.std(Grp3._data)\n",
    "        Grp4._data = (Grp4._data - np.mean(Grp4._data)) / np.std(Grp4._data)\n",
    "    elif args.normalization_type=='lstmPaper':\n",
    "        Grp1._data = (2 * (Grp1._data - np.min(Grp1._data))) \\\n",
    "                        / (np.max(Grp1._data) - np.min(Grp1._data) - 1)\n",
    "        Grp2._data = (2 * (Grp2._data - np.min(Grp2._data))) \\\n",
    "                        / (np.max(Grp2._data) - np.min(Grp2._data) - 1)\n",
    "        Grp3._data = (2 * (Grp3._data - np.min(Grp3._data))) \\\n",
    "                        / (np.max(Grp3._data) - np.min(Grp3._data) - 1)\n",
    "        Grp4._data = (2 * (Grp4._data - np.min(Grp4._data))) \\\n",
    "                        / (np.max(Grp4._data) - np.min(Grp4._data) - 1)\n",
    "    ##==========================================================================\n",
    "    return Grp1, Grp2, Grp3, Grp4, main_ptrn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up a decoder and apply temporal generalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Temporal Generalization\n",
    "\"\"\"\n",
    "def apply_tempGen(args, Grp_data, cv):\n",
    "    le = LabelEncoder()\n",
    "    clf_SVC  = make_pipeline(\n",
    "                          StandardScaler(),\n",
    "                          LinearModel(LinearSVC(random_state=args.random_state,\n",
    "                                                max_iter=args.max_iter)))\n",
    "    X=Grp_data.copy()._data\n",
    "    y=le.fit_transform(Grp_data.copy().metadata.Trgt_Loc_main)\n",
    "\n",
    "    time_gen = GeneralizingEstimator(clf_SVC, scoring=args.scoring,\n",
    "                                     n_jobs=args.n_jobs, verbose=True)\n",
    "    print(np.unique(y))\n",
    "    print(np.unique(Grp_data.copy().metadata.Trgt_Loc_main))\n",
    "\n",
    "    scores = cross_val_multiscore(time_gen, X, y, cv=cv, n_jobs=args.n_jobs)\n",
    "    scores = np.mean(scores, axis=0) #scores with cv\n",
    "    scores_diag = np.diag(scores)\n",
    "    scores_substract = scores_diag - 0.5\n",
    "    scores_pck = (scores.copy(), scores_diag.copy(), scores_substract.copy())\n",
    "\n",
    "    # Without using cv, train and test on the same data\n",
    "    X = Grp_data.copy()._data\n",
    "    y = le.fit_transform(Grp_data.copy().metadata.Trgt_Loc_main)\n",
    "    time_gen.fit(X=X ,y=y)\n",
    "    scores = time_gen.score(X=X, y=y) #scores without cv\n",
    "    scores_diag = np.diag(scores)\n",
    "    scores_pck_fit = (scores.copy(), scores_diag.copy())\n",
    "\n",
    "    return scores_pck, scores_pck_fit\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "def smooth(y, window, mode):\n",
    "    box = np.ones(window)/window\n",
    "    y_smooth = np.convolve(y, box, mode=mode)\n",
    "    return y_smooth\n",
    "\n",
    "def plot_scores(scores):\n",
    "    fig, ax = plt.subplots(1, 1)\n",
    "    plt.tight_layout()\n",
    "    im = ax.imshow(scores, interpolation='lanczos', origin='lower', cmap='RdBu_r',\n",
    "                   extent=subset.times[[0, -1, 0 , -1]], vmin=0., vmax=1.)\n",
    "    ax.set_xlabel('Testing Time (s)')\n",
    "    ax.set_ylabel('Training Time (s)')\n",
    "    ax.set_title('Temporal generalization')\n",
    "    ax.axvline(0, color='k')\n",
    "    ax.axhline(0, color='k')\n",
    "    ax.xaxis.set_ticks_position('bottom')\n",
    "    plt.colorbar(im, ax=ax)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def plot_scores_diag(scores_diag, apply_smooth):\n",
    "    if apply_smooth:\n",
    "        window=50\n",
    "        mode='valid'\n",
    "        scores_diag = smooth(y, window, mode)\n",
    "        print(subset.times.shape)\n",
    "        print(y_smooth.shape)\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot(subset.times, scores_diag, label='score')\n",
    "    ax.axhline(.5, color='k', linestyle='--', label='chance')\n",
    "    ax.set_xlabel('Times')\n",
    "    ax.set_ylabel('AUC')  # Area Under the Curve\n",
    "    ax.legend()\n",
    "    ax.axvline(.0, color='k', linestyle='-')\n",
    "    ax.set_title('Sensor space decoding')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Statistical analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_time_bin(data, indx, sbt):\n",
    "    if sbt==0:\n",
    "        avgs=np.zeros(len(indx))\n",
    "        bs=np.array(np.split(data, indx))\n",
    "        for ii in range(len(indx)):\n",
    "            avgs[ii]=bs[ii].mean()\n",
    "    if sbt==1:\n",
    "         avgs=np.zeros([data.shape[0],len(indx)])\n",
    "         aa=np.zeros(len(indx))\n",
    "         for jj in range(data.shape[0]):\n",
    "             bs=np.array(np.split(data[jj,:], indx))\n",
    "             for ii in range(len(indx)):\n",
    "                 aa[ii]=bs[ii].mean()\n",
    "             avgs[jj,:]=aa\n",
    "    if sbt==2:\n",
    "         avgs=np.zeros([len(indx),len(indx)])\n",
    "         aa=np.zeros(len(indx))\n",
    "         for jj in range(data.shape[0]):\n",
    "             bs1=np.array(np.split(data[jj,:], indx))\n",
    "             bs2=np.array(np.split(data[:,jj], indx))\n",
    "             for ii in range(len(indx)):\n",
    "                 avgs[ii,:]=bs1[ii].mean()\n",
    "                 avgs[:,ii]=bs2[ii].mean()\n",
    "\n",
    "    return avgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_fonts():\n",
    "    from matplotlib.font_manager import FontProperties\n",
    "\n",
    "    font = FontProperties()\n",
    "    font.set_family('serif')\n",
    "    font.set_name('Calibri')\n",
    "    return font\n",
    "\n",
    "def plot_scores_stat(diag_scores, clusts):\n",
    "    font=set_fonts();\n",
    "    [t_obs, clusters, clusters_pv, H0] = clusts\n",
    "    # binned times\n",
    "    times=np.asarray([-0.4,-0.3,-0.2,-0.1,0.1,0.2,0.3,0.4])\n",
    "    extent_times=subset.times[[0, -1, 0, -1]]\n",
    "    \n",
    "    # Plot the diagonal (it's exactly the same as the time-by-time decoding above)\n",
    "    fig, ax = plt.subplots()\n",
    "    plt.tight_layout()\n",
    "    ax.plot(times, diag_scores, label='score')\n",
    "    ax.axhline(.5, color='k', linestyle='--', label='chance')\n",
    "    plt.ylim([0.43,0.65])\n",
    "    ax.axvline(.0, color='k', linestyle='-')\n",
    "\n",
    "    for i_clu, clu_idx in enumerate(clusters):\n",
    "        clu_idx=clu_idx[0]\n",
    "        print(clu_idx)\n",
    "        # unpack cluster information, get unique indices\n",
    "        if clusters_pv[i_clu] <= 0.05:\n",
    "            h = plt.axvspan(times[clu_idx[0]], times[clu_idx[-1] - 1],\n",
    "                            color='r', alpha=0.3)\n",
    "            plt.legend((h, ), ('cluster p-value < 0.05', ))\n",
    "        else:\n",
    "            plt.axvspan(times[clu_idx[0]], times[clu_idx[-1] - 1], color=(0.3, 0.3, 0.3),\n",
    "                        alpha=0.3)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.xlabel('Times',  fontproperties=font, fontsize=12, fontweight='bold')\n",
    "    plt.ylabel('AUC', fontproperties=font, fontsize=12, fontweight='bold')#, labelpad=16,)\n",
    "    plt.title('Decoding over time', fontproperties=font, fontweight='bold', fontsize=16)\n",
    "\n",
    "    plt.legend(fontsize=11)\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statistial analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stat_anal(scores_pck):\n",
    "    indx=[26,51,76,101,126,151,176,201]\n",
    "    score, score_diag, score_subtract = scores_pck\n",
    "    binned_score = do_time_bin(score, indx, 2)\n",
    "    binned_score_diag = do_time_bin(score_diag, indx, 0)\n",
    "    binned_score_subtract = do_time_bin(score_subtract, indx, 0)\n",
    "    print(score_subtract.shape)\n",
    "    score_subtract=score_subtract[:, np.newaxis, np.newaxis] # [:,:, np.newaxis] when added more subjects\n",
    "    print(score_subtract.shape)\n",
    "    t_obs, clusters, clusters_pv, H0 = mne.stats.spatio_temporal_cluster_1samp_test(score_subtract, tail=0)\n",
    "    clust_pck = [t_obs, clusters, clusters_pv, H0]\n",
    "    \n",
    "    return binned_score_diag, clust_pck"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stat analysis for stratified shuffle split result (Alex suggestion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "binned_score_diag_sss, clust_pck_sss = stat_anal(scores_sss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_scores_stat(binned_score_diag_sss, clust_pck_sss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Select some subjects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subj_indx = [ 1,  2,  3,  4,  5,  7,  8,  9, 10, 12, 15, 16, 17, 18, 19, 20, 21,\n",
    "#        22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38,\n",
    "#        39, 42, 43, 44, 45, 46, 47, 48, 51, 52, 53, 55, 56, 57, 58, 59,\n",
    "#        60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74]\n",
    "\n",
    "# selected_subj = [subj_indx[0], subj_indx[29], subj_indx[30], subj_indx[35],\\\n",
    "#                  subj_indx[49], subj_indx[60], 25, 15, 21, 48]\n",
    "\n",
    "# print(selected_subj)\n",
    "\n",
    "selected_subj = [1, 34]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = StratifiedShuffleSplit(n_splits=args.n_splits, random_state=args.random_state)                                                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading ../../data/preprocessed/epochs/aft_ICA_rej/epochs_sec_subj1-afterRejICA-epo.fif ...\n",
      "Isotrak not found\n",
      "    Read a total of 1 projection items:\n",
      "        Average EEG reference (1 x 129) active\n",
      "    Found the data of interest:\n",
      "        t =    -400.00 ...    5000.00 ms\n",
      "        0 CTF compensation matrices available\n",
      "915 matching events found\n",
      "Applying baseline correction (mode: mean)\n",
      "Created an SSP operator (subspace dimension = 1)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-39-b4bdff894b97>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msubj_id\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mselected_subj\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubj_num\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msubj_id\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mGrp1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mGrp2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mGrp3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mGrp4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmain_ptrn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mread_prep_epochs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-23-b2b06fa6ca3a>\u001b[0m in \u001b[0;36mread_prep_epochs\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mfilename_epoch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSAVE_EPOCH_ROOT\u001b[0m \u001b[0;34m+\u001b[0m                          \u001b[0;34m'epochs_sec_subj%s-afterRejICA-epo.fif'\u001b[0m                          \u001b[0;34m%\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubj_num\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     epochs_orig = mne.read_epochs(filename_epoch, proj=True, preload=True,\n\u001b[0;32m---> 10\u001b[0;31m                                   verbose=None)\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mepochs_orig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0msubset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'pred'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'non'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/mne/epochs.py\u001b[0m in \u001b[0;36mread_epochs\u001b[0;34m(fname, proj, preload, verbose)\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/mne/utils/_logging.py\u001b[0m in \u001b[0;36mverbose\u001b[0;34m(function, *args, **kwargs)\u001b[0m\n\u001b[1;32m     72\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0muse_log_level\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mverbose_level\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/mne/epochs.py\u001b[0m in \u001b[0;36mread_epochs\u001b[0;34m(fname, proj, preload, verbose)\u001b[0m\n\u001b[1;32m   2329\u001b[0m         \u001b[0mThe\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2330\u001b[0m     \"\"\"\n\u001b[0;32m-> 2331\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mEpochsFIF\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreload\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2332\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2333\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/mne/epochs.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, fname, proj, preload, verbose)\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/mne/utils/_logging.py\u001b[0m in \u001b[0;36mverbose\u001b[0;34m(function, *args, **kwargs)\u001b[0m\n\u001b[1;32m     72\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0muse_log_level\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mverbose_level\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/mne/epochs.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, fname, proj, preload, verbose)\u001b[0m\n\u001b[1;32m   2416\u001b[0m         (info, data, events, event_id, tmin, tmax, metadata, baseline,\n\u001b[1;32m   2417\u001b[0m          selection, drop_log, _) = \\\n\u001b[0;32m-> 2418\u001b[0;31m             \u001b[0m_concatenate_epochs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mep_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwith_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpreload\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madd_offset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2419\u001b[0m         \u001b[0;31m# we need this uniqueness for non-preloaded data to work properly\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2420\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevents\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevents\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/mne/epochs.py\u001b[0m in \u001b[0;36m_concatenate_epochs\u001b[0;34m(epochs_list, with_data, add_offset)\u001b[0m\n\u001b[1;32m   2675\u001b[0m             \u001b[0mmetadata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmetadata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2676\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mwith_data\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2677\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2678\u001b[0m     return (info, data, events, event_id, tmin, tmax, metadata, baseline,\n\u001b[1;32m   2679\u001b[0m             selection, drop_log, verbose)\n",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mconcatenate\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "avgp1_score=[]\n",
    "avgp2_score=[]\n",
    "\n",
    "avgp1_diag=[]\n",
    "avgp2_diag=[]\n",
    "\n",
    "avgp1_substract=[]\n",
    "avgp2_substract=[]\n",
    "\n",
    "\n",
    "for subj_id in selected_subj:\n",
    "    args.subj_num = subj_id\n",
    "    Grp1, Grp2, Grp3, Grp4, main_ptrn = read_prep_epochs(args)\n",
    "    \n",
    "    sc_pck_G1, sc_pck_fit_G1 = apply_tempGen(args, Grp_data, cv)\n",
    "    sc_pck_G2, sc_pck_fit_G2 = apply_tempGen(args, Grp_data, cv)\n",
    "    sc_pck_G3, sc_pck_fit_G3 = apply_tempGen(args, Grp_data, cv)\n",
    "    sc_pck_G4, sc_pck_fit_G4 = apply_tempGen(args, Grp_data, cv)\n",
    "    \n",
    "    # unpack them\n",
    "    score_G1, score_diag_G1, score_subtract_G1 = sc_pck_G1\n",
    "    score_G2, score_diag_G2, score_subtract_G2 = sc_pck_G2\n",
    "    score_G3, score_diag_G3, score_subtract_G3 = sc_pck_G3\n",
    "    score_G4, score_diag_G4, score_subtract_G4 = sc_pck_G4\n",
    "    \n",
    "    # integrate them for all subjects\n",
    "    if main_ptrn == 1:\n",
    "        avgp1_score.append(sc_pck_G1)\n",
    "        avgp1_diag.append(score_diag_G1)\n",
    "        avgp1_substract.append(score_subtract_G1)\n",
    "        \n",
    "        avgp1_score.append(sc_pck_G2)\n",
    "        avgp1_diag.append(score_diag_G2)\n",
    "        avgp1_substract.append(score_subtract_G2)\n",
    "        \n",
    "        avgp1_score.append(sc_pck_G3)\n",
    "        avgp1_diag.append(score_diag_G3)\n",
    "        avgp1_substract.append(score_subtract_G3)\n",
    "        \n",
    "        avgp1_score.append(sc_pck_G4)\n",
    "        avgp1_diag.append(score_diag_G4)\n",
    "        avgp1_substract.append(score_subtract_G4)\n",
    "    elif man_ptrn == 2:\n",
    "        avgp2_score.append(sc_pck_G1)\n",
    "        avgp2_diag.append(score_diag_G1)\n",
    "        avgp2_substract.append(score_subtract_G1)\n",
    "        \n",
    "        avgp2_score.append(sc_pck_G2)\n",
    "        avgp2_diag.append(score_diag_G2)\n",
    "        avgp2_substract.append(score_subtract_G2)\n",
    "        \n",
    "        avgp2_score.append(sc_pck_G3)\n",
    "        avgp2_diag.append(score_diag_G3)\n",
    "        avgp2_substract.append(score_subtract_G3)\n",
    "        \n",
    "        avgp2_score.append(sc_pck_G4)\n",
    "        avgp2_diag.append(score_diag_G4)\n",
    "        avgp2_substract.append(score_subtract_G4)\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# General decoder params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "nSplits=5\n",
    "scoring='roc_auc'\n",
    "randState=0\n",
    "maxIter=10000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up a decoder\n",
    "\n",
    "\n",
    "## Using StandardScaler from sklearn.preprocessing:\n",
    "### mne.decoding.Scaler\n",
    "    scales each channel \n",
    "    using mean and standard deviation computed across all of its time points and epochs. \n",
    "###  sklearn.preprocessing.StandardScaler offered by scikit-learn\n",
    "    These scale each classification feature, e.g. each time point for each channel, \n",
    "    with mean and standard deviation computed across epochs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
