{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check if saving and everything else has been correctly done after run the script for prestim later none filter on eCortex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mne\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from mne.datasets import sample\n",
    "from mne.decoding import cross_val_multiscore, LinearModel, GeneralizingEstimator, Scaler, \\\n",
    "                         Vectorizer\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_score, StratifiedShuffleSplit, \\\n",
    "                                    RepeatedStratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "import argparse\n",
    "\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class arguments:\n",
    "    def __init__(self, cond_block, cond_decoding, applyBaseline_bool, mtdt_feat, occ_channels):\n",
    "        self.cond_block = cond_block #'later' #{early,later}\n",
    "        self.cond_decoding = cond_decoding #'removeevoked' #{none,removeevoked,resampled}\n",
    "        self.applyBaseline_bool = applyBaseline_bool #'False'\n",
    "        self.mtdt_feat = mtdt_feat\n",
    "        self.occ_channels = occ_channels\n",
    "        \n",
    "        self.SAVE_EPOCH_ROOT = '../../../data/version5.2/preprocessed/epochs/aft_ICA_rej/'\n",
    "        self.SAVE_RESULT_ROOT = '../../../results/temp_gen/blanca/'\n",
    "        self.cond_filter ='none' # {none,non_symm}\n",
    "        self.cond_time = 'prestim' #{prestim,poststim}\n",
    "        self.subj_num = 1\n",
    "        self.pre_tmin = -0.4\n",
    "        self.pre_tmax = 0.05\n",
    "        self.post_tmin = 0.05\n",
    "        self.post_tmax = 0.45\n",
    "        self.num_classes = 2\n",
    "        self.normalization_type = 'normal'# {normal,lstmPaper}\n",
    "        self.gen_rand_perm = 0\n",
    "        self.null_max_iter = 10000\n",
    "        self.loop_null_iter = 5\n",
    "        self.gen_decoder_scores = 1\n",
    "        self.random_state = 42 \n",
    "        self.max_iter = 10000\n",
    "        self.n_jobs = 1\n",
    "        self.scoring = 'roc_auc'\n",
    "        self.n_splits = '_3k'\n",
    "        self.smooth_lvl = 55\n",
    "        self.occ_channels = 'True'\n",
    "        print(self.SAVE_RESULT_ROOT)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Reading and preparing epoch data to create each 4 grous and 2 pattern\n",
    "\"\"\"\n",
    "import mne\n",
    "import numpy as np\n",
    "from scipy.signal import savgol_filter\n",
    "\n",
    "def read_prep_epochs(args):\n",
    "\n",
    "\n",
    "    if args.applyBaseline_bool:\n",
    "        filename_epoch = args.SAVE_EPOCH_ROOT + \\\n",
    "                         'epochs_sec_applyBaseline_subj%s-afterRejICA-epo.fif' \\\n",
    "                          %args.subj_num\n",
    "    else:\n",
    "        filename_epoch = args.SAVE_EPOCH_ROOT + \\\n",
    "                         'epochs_sec_subj%s-afterRejICA-epo.fif' \\\n",
    "                         %args.subj_num\n",
    "    epochs_orig = mne.read_epochs(filename_epoch, proj=True, preload=True,\n",
    "                                  verbose=None)\n",
    "    epochs = epochs_orig.copy()\n",
    "    subset = epochs['pred']['non'].copy()\n",
    "    subset = subset.pick_types(eeg=True)\n",
    "    if (args.cond_decoding=='removeevoked'):\n",
    "        # REMOVE EVOKED RESP.\n",
    "        subset.subtract_evoked()    # remove evoked response\n",
    "    elif (args.cond_decoding=='resampled'):\n",
    "        # RESAMPLE\n",
    "        subset = subset.resample(args.n_resampling, npad='auto')\n",
    "    else:\n",
    "        pass\n",
    "    ##==========================================================================\n",
    "    # Select EEG channels of the back\n",
    "    if args.occ_channels:\n",
    "        picks_back_brain = ['E56', 'E63', 'E68', 'E73', 'E81', 'E88', 'E94', 'E99', 'E107',\n",
    "        'E57', 'E64', 'E69', 'E74', 'E82', 'E89', 'E95', 'E100',\n",
    "        'E50', 'E58', 'E65', 'E70', 'E75', 'E83', 'E90', 'E96', 'E101',\n",
    "        'E51', 'E59', 'E66', 'E71', 'E76', 'E84', 'E91', 'E97',\n",
    "        'E52', 'E60', 'E67', 'E72', 'E77', 'E85', 'E92'\n",
    "        'E53', 'E61', 'E62', 'E78','E86'\n",
    "        ]\n",
    "        subset = subset.pick_types(eeg=True, selection=picks_back_brain)\n",
    "    ##==========================================================================\n",
    "    if subset['Block==7'].metadata.Ptrn_Type.values.shape[0]>0:\n",
    "       main_ptrn = subset['Block==7'].metadata.Ptrn_Type.values[0]\n",
    "    else:\n",
    "       main_ptrn = subset['Block==8'].metadata.Ptrn_Type.values[0]\n",
    "    ##==========================================================================\n",
    "    if args.cond_block=='early': #block 3-6\n",
    "        subset = subset['Block<7'].copy()\n",
    "        subset = subset['Block>2'].copy()\n",
    "    elif args.cond_block=='later':#block 7-10\n",
    "        subset = subset['Block<11'].copy()\n",
    "        subset = subset['Block>6'].copy()\n",
    "    elif args.cond_block=='diff':\n",
    "        se = subset['Block<7'].copy() # early blocks\n",
    "        se = se['Block>2'].copy()\n",
    "        print('earlier blocks shape:\\n')\n",
    "        print(se._data.shape)\n",
    "        sl= subset['Block<11'].copy() # later blocks\n",
    "        sl = sl['Block>6'].copy()\n",
    "        print('later blocks shape:\\n')\n",
    "        print(sl._data.shape)\n",
    "        subset._data=sl._data-se._data\n",
    "\n",
    "    ##==========================================================================\n",
    "    subset= subset.crop(tmin=-0.45, tmax=0.45)\n",
    "    # if (args.cond_time=='prestim'):\n",
    "    #     subset= subset.crop(tmin=-0.4, tmax=0.05)\n",
    "    # if (args.cond_time=='poststim'):\n",
    "    #     subset= subset.crop(tmin=0.05, tmax=0.45)\n",
    "    # print('Shape of data after crop time is\\n :')\n",
    "    print(subset._data.shape)\n",
    "    ##==========================================================================\n",
    "    # Group data based on the previous trial\n",
    "     # Group data based on the current main loc\n",
    "    Grp1 = subset['%s==1' %(args.mtdt_feat)].copy()\n",
    "    Grp2 = subset['%s==2' %(args.mtdt_feat)].copy()\n",
    "    Grp3 = subset['%s==3' %(args.mtdt_feat)].copy()\n",
    "    Grp4 = subset['%s==4' %(args.mtdt_feat)].copy()\n",
    "\n",
    "    # Grp1 = subset['Trgt_Loc_prev==1'].copy()\n",
    "    # Grp2 = subset['Trgt_Loc_prev==2'].copy()\n",
    "    # Grp3 = subset['Trgt_Loc_prev==3'].copy()\n",
    "    # Grp4 = subset['Trgt_Loc_prev==4'].copy()\n",
    "    if main_ptrn==1:\n",
    "        Grp1 = Grp1['Trgt_Loc_main!=4'].copy()\n",
    "        Grp2 = Grp2['Trgt_Loc_main!=1'].copy()\n",
    "        Grp3 = Grp3['Trgt_Loc_main!=2'].copy()\n",
    "        Grp4 = Grp4['Trgt_Loc_main!=3'].copy()\n",
    "    ##==========================================================================\n",
    "    frequencies = np.arange(3, 13, 2)\n",
    "    if args.cond_decoding=='non_symm':\n",
    "        Grp1 = apply_nonSymm_filter(Grp1, frequencies)\n",
    "        Grp2 = apply_nonSymm_filter(Grp2, frequencies)\n",
    "        Grp3 = apply_nonSymm_filter(Grp3, frequencies)\n",
    "        Grp4 = apply_nonSymm_filter(Grp4, frequencies)\n",
    "    ##==========================================================================\n",
    "    inds = np.zeros((4,1))\n",
    "    for iind in range(4):\n",
    "        inds[iind] = subset['%s==%s' %(args.mtdt_feat, iind+1)]._data.shape[0]\n",
    "\n",
    "    ind1=int(min(inds))\n",
    "    ind2=subset['%s==1' %(args.mtdt_feat)]._data.shape[1]\n",
    "    ind3=subset['%s==1' %(args.mtdt_feat)]._data.shape[2]\n",
    "    print('minimum ind across four groups: ', ind1)\n",
    "    ##==========================================================================\n",
    "    # Equalize the number of each group\n",
    "    # Grp1._data = Grp1._data[:ind1,:,:]\n",
    "    # Grp2._data = Grp2._data[:ind1,:,:]\n",
    "    # Grp3._data = Grp3._data[:ind1,:,:]\n",
    "    # Grp4._data = Grp4._data[:ind1,:,:]\n",
    "    ##==========================================================================\n",
    "    print('the pattern for this subj is :=====================================')\n",
    "    print(main_ptrn)\n",
    "    print('          ')\n",
    "    print('===================================================================')\n",
    "    ##==========================================================================\n",
    "    # Normalizing the data for each subject\n",
    "    if args.normalization_type=='normal':\n",
    "        Grp1._data = (Grp1._data - np.mean(Grp1._data)) / np.std(Grp1._data)\n",
    "        Grp2._data = (Grp2._data - np.mean(Grp2._data)) / np.std(Grp2._data)\n",
    "        Grp3._data = (Grp3._data - np.mean(Grp3._data)) / np.std(Grp3._data)\n",
    "        Grp4._data = (Grp4._data - np.mean(Grp4._data)) / np.std(Grp4._data)\n",
    "    elif args.normalization_type=='lstmPaper':\n",
    "        Grp1._data = (2 * (Grp1._data - np.min(Grp1._data))) \\\n",
    "                        / (np.max(Grp1._data) - np.min(Grp1._data) - 1)\n",
    "        Grp2._data = (2 * (Grp2._data - np.min(Grp2._data))) \\\n",
    "                        / (np.max(Grp2._data) - np.min(Grp2._data) - 1)\n",
    "        Grp3._data = (2 * (Grp3._data - np.min(Grp3._data))) \\\n",
    "                        / (np.max(Grp3._data) - np.min(Grp3._data) - 1)\n",
    "        Grp4._data = (2 * (Grp4._data - np.min(Grp4._data))) \\\n",
    "                        / (np.max(Grp4._data) - np.min(Grp4._data) - 1)\n",
    "    ##==========================================================================\n",
    "    Grps_dt = np.zeros((4, ind1, ind2, ind3))\n",
    "    Grps_dt[0,:,:,:]=Grp1._data[:ind1,:,:]\n",
    "    Grps_dt[1,:,:,:]=Grp2._data[:ind1,:,:]\n",
    "    Grps_dt[2,:,:,:]=Grp3._data[:ind1,:,:]\n",
    "    Grps_dt[3,:,:,:]=Grp4._data[:ind1,:,:]\n",
    "    Grps_avg = np.mean(Grps_dt, axis=1)\n",
    "    ##==========================================================================\n",
    "    # smoothing data\n",
    "    evk_data = np.mean(Grps_avg, axis=1)\n",
    "    smooth_evk = np.zeros((5, evk_data.shape[1]))\n",
    "    smooth_evk[0,:] = savgol_filter(evk_data[0,:], args.smooth_lvl, 3)\n",
    "    smooth_evk[1,:] = savgol_filter(evk_data[1,:], args.smooth_lvl, 3)\n",
    "    smooth_evk[2,:] = savgol_filter(evk_data[2,:], args.smooth_lvl, 3)\n",
    "    smooth_evk[3,:] = savgol_filter(evk_data[3,:], args.smooth_lvl, 3)\n",
    "    smooth_evk[4,:] = savgol_filter(np.mean(evk_data, 0), args.smooth_lvl, 3)\n",
    "    ##==========================================================================\n",
    "    # return Grp1, Grp2, Grp3, Grp4, main_ptrn\n",
    "    return Grp1, Grp2, Grp3, Grp4, Grps_dt, Grps_avg, smooth_evk, main_ptrn\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utils functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load nested list results using pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# selected_subj_P1 = [1, 2, 3, 4, 5, 7, 8, 9, 10, 12, 15, 16, \\\n",
    "#                     42, 43, 44, 45, 47, 48, 51, 52, 53, \\\n",
    "#                     55, 56, 57, 58, 59, 60, 61, 62, 63, 64, \\\n",
    "#                     66, 67, 68, 69, 71, 72, 73, 74]\n",
    "# selected_subj_P2 = [18, 19, 20, 21, 23, 24, 26, 28, 29, 30, \\\n",
    "#                    31, 32, 33, 34, 35, 36, 38, 39]\n",
    "\n",
    "selected_subj_P1_E = [1, 8, 9, 16, 43, 45, 46, 48, 59, 60, 63, \\\n",
    "                      66, 68] # Early\n",
    "selected_subj_P1_L = [4, 8, 10, 15, 17, 43, 44, 45, 48, 52, 59, \\\n",
    "                      63, 64] # Later\n",
    "selected_subj_P1_B = [8, 43, 45, 48, 59, 63] # Both Early and Later\n",
    "\n",
    "\n",
    "\n",
    "selected_subj_P2_E = [26, 28, 35, 38, 39]\n",
    "selected_subj_P2_L = [20, 22]\n",
    "selected_subj_P2_B = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_subj_scores(args, subj_p1, subj_p2):\n",
    "    avgp1_sc=[]\n",
    "    avgp2_sc=[]\n",
    "    avgp1_diag=[]\n",
    "    avgp2_diag=[]\n",
    "\n",
    "    avgp1_sc_fit=[]\n",
    "    avgp2_sc_fit=[]\n",
    "    avgp1_diag_fit=[]\n",
    "    avgp2_diag_fit=[]\n",
    "\n",
    "    for subj_id in subj_p1:\n",
    "        main_ptrn = 1\n",
    "        args.subj_num = subj_id\n",
    "\n",
    "        fn_str_sbj='scores_timeGen_%sBlocks_%sFilter_PrePost_decod%s_bsline%s%s_Subj_%s' \\\n",
    "                %(args.cond_block, args.cond_filter, \\\n",
    "                args.cond_decoding, args.applyBaseline_bool, \\\n",
    "                args.n_splits, args.subj_num)\n",
    "\n",
    "        fn_str = args.SAVE_RESULT_ROOT + 'avgP%s_' %(main_ptrn) + fn_str_sbj\n",
    "\n",
    "\n",
    "        with open(fn_str, 'rb') as f:\n",
    "            sc_subj_pck = pickle.load(f)\n",
    "\n",
    "        avgp1_sc.append(sc_subj_pck[0])\n",
    "        avgp1_diag.append(sc_subj_pck[1])\n",
    "\n",
    "        avgp1_sc_fit.append(sc_subj_pck[2])\n",
    "        avgp1_diag_fit.append(sc_subj_pck[3])\n",
    "\n",
    "    for subj_id in subj_p2:\n",
    "        main_ptrn = 2\n",
    "        args.subj_num = subj_id\n",
    "\n",
    "        fn_str_sbj='scores_timeGen_%sBlocks_%sFilter_PrePost_decod%s_bsline%s%s_Subj_%s' \\\n",
    "                %(args.cond_block, args.cond_filter, \\\n",
    "                args.cond_decoding, args.applyBaseline_bool, \\\n",
    "                args.n_splits, args.subj_num)\n",
    "\n",
    "        fn_str = args.SAVE_RESULT_ROOT + 'avgP%s_' %(main_ptrn) + fn_str_sbj\n",
    "\n",
    "\n",
    "        with open(fn_str, 'rb') as f:\n",
    "            sc_subj_pck = pickle.load(f)\n",
    "\n",
    "\n",
    "        avgp2_sc.append(sc_subj_pck[0])\n",
    "        avgp2_diag.append(sc_subj_pck[1])\n",
    "\n",
    "        avgp2_sc_fit.append(sc_subj_pck[2])\n",
    "        avgp2_diag_fit.append(sc_subj_pck[3])\n",
    "\n",
    "\n",
    "    print(np.asarray(avgp1_diag).shape)\n",
    "    print(np.asarray(avgp2_diag).shape)\n",
    "\n",
    "    p1=np.asarray(avgp1_sc)\n",
    "    p2=np.asarray(avgp2_sc)\n",
    "    \n",
    "    p1d=np.asarray(avgp1_diag)\n",
    "    p2d=np.asarray(avgp2_diag)\n",
    "\n",
    "\n",
    "    print('p1 shape', p1.shape[0], 'p2 shape', p2.shape[0])\n",
    "    \n",
    "    if p2.shape[0]==0:\n",
    "        p=p1\n",
    "        pd=p1d\n",
    "        print('There was no subjects for pattern 2')\n",
    "    elif p1.shape[0]==0:\n",
    "        p=p2\n",
    "        pd=p2d\n",
    "        print('There was no subjects for pattern 1')\n",
    "    else:\n",
    "        p=np.concatenate((p1, p2), axis=0)\n",
    "        pd=np.concatenate((p1d, p2d), axis=0)\n",
    "\n",
    "    scores_pck_p = [p, pd]\n",
    "    \n",
    "    return scores_pck_p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_subj_rand_scores(args, subj_p1, subj_p2):\n",
    "    avgp1_sc=[]\n",
    "    avgp2_sc=[]\n",
    "    avgp1_diag=[]\n",
    "    avgp2_diag=[]\n",
    "\n",
    "    avgp1_sc_fit=[]\n",
    "    avgp2_sc_fit=[]\n",
    "    avgp1_diag_fit=[]\n",
    "    avgp2_diag_fit=[]\n",
    "\n",
    "    for subj_id in subj_p1:\n",
    "        main_ptrn = 1\n",
    "        args.subj_num = subj_id\n",
    "\n",
    "        fn_str_sbj='scores_timeGen_%sBlocks_%sFilter_PrePost_decod%s_bsline%s%s_%s_Subj_%s' \\\n",
    "                %(args.cond_block, args.cond_filter, \\\n",
    "                  args.cond_decoding, args.applyBaseline_bool, \\\n",
    "                  args.n_splits, args.mtdt_feat, args.subj_num)\n",
    "\n",
    "        fn_str = args.SAVE_RESULT_ROOT + 'rand_avgP%s_' %(main_ptrn) + fn_str_sbj\n",
    "\n",
    "\n",
    "        with open(fn_str, 'rb') as f:\n",
    "            sc_subj_pck = pickle.load(f)\n",
    "\n",
    "        avgp1_sc.append(sc_subj_pck[0])\n",
    "        avgp1_diag.append(sc_subj_pck[1])\n",
    "\n",
    "        avgp1_sc_fit.append(sc_subj_pck[2])\n",
    "        avgp1_diag_fit.append(sc_subj_pck[3])\n",
    "\n",
    "    for subj_id in subj_p2:\n",
    "        main_ptrn = 2\n",
    "        args.subj_num = subj_id\n",
    "\n",
    "        fn_str_sbj='scores_timeGen_%sBlocks_%sFilter_PrePost_decod%s_bsline%s%s_%s_Subj_%s' \\\n",
    "                %(args.cond_block, args.cond_filter, \\\n",
    "                  args.cond_decoding, args.applyBaseline_bool, \\\n",
    "                  args.n_splits, args.mtdt_feat, args.subj_num)\n",
    "\n",
    "        fn_str = args.SAVE_RESULT_ROOT + 'rand_avgP%s_' %(main_ptrn) + fn_str_sbj\n",
    "\n",
    "\n",
    "        with open(fn_str, 'rb') as f:\n",
    "            sc_subj_pck = pickle.load(f)\n",
    "\n",
    "\n",
    "        avgp2_sc.append(sc_subj_pck[0])\n",
    "        avgp2_diag.append(sc_subj_pck[1])\n",
    "\n",
    "        avgp2_sc_fit.append(sc_subj_pck[2])\n",
    "        avgp2_diag_fit.append(sc_subj_pck[3])\n",
    "\n",
    "\n",
    "    print(np.asarray(avgp1_diag).shape)\n",
    "    print(np.asarray(avgp2_diag).shape)\n",
    "\n",
    "    p1=np.asarray(avgp1_sc)\n",
    "    p2=np.asarray(avgp2_sc)\n",
    "    \n",
    "    p1d=np.asarray(avgp1_diag)\n",
    "    p2d=np.asarray(avgp2_diag)\n",
    "\n",
    "\n",
    "    print('p1 shape', p1.shape[0], 'p2 shape', p2.shape[0])\n",
    "    \n",
    "    if p2.shape[0]==0:\n",
    "        p=p1\n",
    "        pd=p1d\n",
    "        print('There was no subjects for pattern 2')\n",
    "    elif p1.shape[0]==0:\n",
    "        p=p2\n",
    "        pd=p2d\n",
    "        print('There was no subjects for pattern 1')\n",
    "    else:\n",
    "        p=np.concatenate((p1, p2), axis=0)\n",
    "        pd=np.concatenate((p1d, p2d), axis=0)\n",
    "\n",
    "\n",
    "\n",
    "    scores_pck_p = [p, pd]\n",
    "    \n",
    "    return scores_pck_p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_subj_ERPs(args, subj_p1, subj_p2):\n",
    "\n",
    "    ERP_p1=[]\n",
    "    ERP_p2=[]\n",
    "    ind_Grps_dt=4\n",
    "    \n",
    "    for subj_id in subj_p1:\n",
    "        print('subject id:', subj_id)\n",
    "        main_ptrn = 1\n",
    "        args.subj_num = subj_id\n",
    "\n",
    "        fn_str_sbj='%sBlocks_%sFilter_PrePost_decod%s_bsline%s_%sChann_%s_Subj_%s' \\\n",
    "                    %(args.cond_block, args.cond_filter, \\\n",
    "                      args.cond_decoding, args.applyBaseline_bool, \\\n",
    "                      args.occ_channels, args.mtdt_feat, args.subj_num)\n",
    "\n",
    "        fn_str = args.SAVE_RESULT_ROOT + 'ERP_P%s_' %(main_ptrn) + fn_str_sbj\n",
    "\n",
    "\n",
    "        with open(fn_str, 'rb') as f:\n",
    "            erps = pickle.load(f)\n",
    "            \n",
    "        erps_avgG1 = np.mean(erps[ind_Grps_dt].copy(), axis=1)\n",
    "        ERP_p1.append(erps_avgG1.copy())\n",
    "\n",
    "\n",
    "    for subj_id in subj_p2:\n",
    "        main_ptrn = 2\n",
    "        args.subj_num = subj_id\n",
    "\n",
    "        fn_str_sbj='%sBlocks_%sFilter_PrePost_decod%s_bsline%s_%sChann_%s_Subj_%s' \\\n",
    "                    %(args.cond_block, args.cond_filter, \\\n",
    "                      args.cond_decoding, args.applyBaseline_bool, \\\n",
    "                      args.occ_channels, args.mtdt_feat, args.subj_num)\n",
    "\n",
    "        fn_str = args.SAVE_RESULT_ROOT + 'ERP_P%s_' %(main_ptrn) + fn_str_sbj\n",
    "\n",
    "\n",
    "        with open(fn_str, 'rb') as f:\n",
    "            erps = pickle.load(f)\n",
    "\n",
    "        erps_avgG2 = np.mean(erps[ind_Grps_dt].copy(), axis=1)\n",
    "        ERP_p2.append(erps_avgG2.copy())\n",
    "        \n",
    "    print('p1 subjects:',np.asarray(ERP_p1).shape)\n",
    "    print('p2 subjects', np.asarray(ERP_p2).shape)\n",
    "\n",
    "    p1=np.asarray(ERP_p1)\n",
    "    p2=np.asarray(ERP_p2)\n",
    "    \n",
    "    \n",
    "    if p2.shape[0]==0:\n",
    "        p=p1\n",
    "        print('There was no subjects for pattern 2')\n",
    "    elif p1.shape[0]==0:\n",
    "        p=p2\n",
    "        print('There was no subjects for pattern 1')\n",
    "    else:\n",
    "        p=np.concatenate((p1, p2), axis=0)\n",
    "    \n",
    "\n",
    "    subset = erps[0].copy()\n",
    "    \n",
    "    return p, subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def smooth(y, window, mode):\n",
    "    box = np.ones(window)/window\n",
    "    y_smooth = np.convolve(y, box, mode=mode)\n",
    "    return y_smooth\n",
    "\n",
    "def plot_scores(scores):\n",
    "    font=set_fonts();\n",
    "    fig, ax = plt.subplots(1, 1)\n",
    "    plt.tight_layout()\n",
    "    im = ax.imshow(scores, cmap='RdBu_r', origin='lower', interpolation='lanczos', \n",
    "                   extent=subset.times[[0, -1, 0 , -1]])#, vmin=0., vmax=1)\n",
    "        \n",
    "    ax.set_xlabel('Testing Time (s)', fontproperties=font, fontsize=12, fontweight='bold')\n",
    "    ax.set_ylabel('Training Time (s)', fontproperties=font, fontsize=12, fontweight='bold')\n",
    "    ax.set_title('Temporal generalization for %s subjects ' %(num_tot_subjects), fontproperties=font, fontweight='bold', fontsize=16)\n",
    "    ax.axvline(0, color='k')\n",
    "    ax.axhline(0, color='k')\n",
    "    ax.xaxis.set_ticks_position('bottom')\n",
    "    plt.colorbar(im, ax=ax)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def plot_scores_diag(scores_diag, apply_smooth):\n",
    "    font=set_fonts();\n",
    "    if apply_smooth:\n",
    "        indx=[16,31,46,61,76,91,106,121,136,151,166,181,196] #every 15 samples -> 15*4=60ms\n",
    "        scores_diag = do_time_bin(scores_diag.copy(),indx, sbt=0)\n",
    "        plot_times=subset.times[indx]\n",
    "    else:\n",
    "        plot_times = subset.times\n",
    "        \n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot(plot_times, scores_diag, label='score')\n",
    "    ax.axhline(.5, color='k', linestyle='--', label='chance')\n",
    "    ax.set_xlabel('Times', fontproperties=font, fontsize=12, fontweight='bold')\n",
    "    ax.set_ylabel('AUC', fontproperties=font, fontsize=12, fontweight='bold')  # Area Under the Curve\n",
    "    ax.legend()\n",
    "    ax.axvline(.0, color='k', linestyle='-')\n",
    "    ax.set_title('Sensor space decoding for %s subjects ' %(num_tot_subjects), fontproperties=font, fontweight='bold', fontsize=16)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_time_bin(data, indx, sbt):\n",
    "    if sbt==0:\n",
    "        avgs=np.zeros(len(indx))\n",
    "        bs=np.array(np.split(data, indx))\n",
    "        for ii in range(len(indx)):\n",
    "            avgs[ii]=bs[ii].mean()\n",
    "    if sbt==1:\n",
    "         avgs=np.zeros([data.shape[0],len(indx)])\n",
    "         aa=np.zeros(len(indx))\n",
    "         for jj in range(data.shape[0]):\n",
    "             bs=np.array(np.split(data[jj,:], indx))\n",
    "             for ii in range(len(indx)):\n",
    "                 aa[ii]=bs[ii].mean()\n",
    "             avgs[jj,:]=aa\n",
    "    if sbt==2:\n",
    "         avgs=np.zeros([len(indx),len(indx)])\n",
    "         aa=np.zeros(len(indx))\n",
    "         for jj in range(data.shape[0]):\n",
    "             bs1=np.array(np.split(data[jj,:], indx))\n",
    "             bs2=np.array(np.split(data[:,jj], indx))\n",
    "             for ii in range(len(indx)):\n",
    "                 avgs[ii,:]=bs1[ii].mean()\n",
    "                 avgs[:,ii]=bs2[ii].mean()\n",
    "\n",
    "    return avgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_fonts():\n",
    "    from matplotlib.font_manager import FontProperties\n",
    "\n",
    "    font = FontProperties()\n",
    "    font.set_family('serif')\n",
    "    font.set_name('Calibri')\n",
    "    return font\n",
    "\n",
    "def plot_scores_stat(diag_scores, clusts, num_tot_subjects):\n",
    "    print(diag_scores.shape)\n",
    "   \n",
    "    indx=[16,31,46,61,76,91,106,121,136,151,166,181,196] #every 15 samples -> 15*4=60ms\n",
    "    diag_scores = do_time_bin(diag_scores.copy(),indx, sbt=0)\n",
    "    plot_times=subset.times[indx]\n",
    "        \n",
    "    font=set_fonts()\n",
    "    [t_obs, clusters, clusters_pv, H0] = clusts\n",
    "    \n",
    "    # Plot the diagonal (it's exactly the same as the time-by-time decoding above)\n",
    "    fig, ax = plt.subplots()\n",
    "    plt.tight_layout()\n",
    "    ax.plot(plot_times, diag_scores, label='score')\n",
    "    ax.axhline(.5, color='k', linestyle='--', label='chance')\n",
    "    plt.ylim([0.45,0.55])\n",
    "    ax.axvline(.0, color='k', linestyle='-')\n",
    "\n",
    "    for i_clu, clu_idx in enumerate(clusters):\n",
    "        clu_idx=clu_idx[0]\n",
    "        print(clu_idx)\n",
    "        # unpack cluster information, get unique indices\n",
    "        if clusters_pv[i_clu] <= 0.05:\n",
    "            h = plt.axvspan(plot_times[clu_idx[0]], plot_times[clu_idx[-1] - 1],\n",
    "                            color='r', alpha=0.3)\n",
    "            plt.legend((h, ), ('cluster p-value < 0.05', ))\n",
    "        else:\n",
    "            plt.axvspan(plot_times[clu_idx[0]], plot_times[clu_idx[-1] - 1], color=(0.3, 0.3, 0.3),\n",
    "                        alpha=0.3)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.xlabel('Times',  fontproperties=font, fontsize=12, fontweight='bold')\n",
    "    plt.ylabel('AUC', fontproperties=font, fontsize=12, fontweight='bold')#, labelpad=16,)\n",
    "    plt.title('Decoding over time for %s subjects ' %(num_tot_subjects), fontproperties=font, fontweight='bold', fontsize=16)\n",
    "\n",
    "    plt.legend(fontsize=11)\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stat_anal(score, score_diag):\n",
    "    indx=[16,31,46,61,76,91,106,121,136,151,166,181,196] #every 15 samples -> 15*4=60ms\n",
    "    score_subtract = score_diag - 0.5\n",
    "    binned_score = do_time_bin(score, indx, 2)\n",
    "    binned_score_diag = do_time_bin(score_diag, indx, 0)\n",
    "    binned_score_subtract = do_time_bin(score_subtract, indx, 0)\n",
    "    print(score_subtract.shape)\n",
    "    score_subtract=score_subtract[:, np.newaxis, np.newaxis] # [:,:, np.newaxis] when added more subjects\n",
    "    print(score_subtract.shape)\n",
    "    t_obs, clusters, clusters_pv, H0 = mne.stats.spatio_temporal_cluster_1samp_test(score_subtract, tail=0)\n",
    "    clust_pck = [t_obs, clusters, clusters_pv, H0]\n",
    "    \n",
    "    return binned_score_diag, clust_pck"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Different conditions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## cond1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../../../results/temp_gen/blanca/\n",
      "Reading ../../../data/version5.2/preprocessed/epochs/aft_ICA_rej/epochs_sec_applyBaseline_subj1-afterRejICA-epo.fif ...\n",
      "Isotrak not found\n",
      "    Read a total of 1 projection items:\n",
      "        Average EEG reference (1 x 129) active\n",
      "    Found the data of interest:\n",
      "        t =    -400.00 ...    5000.00 ms\n",
      "        0 CTF compensation matrices available\n",
      "1197 matching events found\n",
      "Applying baseline correction (mode: mean)\n",
      "Adding metadata with 16 columns\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "Subtracting Evoked from Epochs\n",
      "[done]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-3-b49f8a41559c>:67: RuntimeWarning: tmin is not in epochs time interval. tmin is set to epochs.tmin\n",
      "  subset= subset.crop(tmin=-0.45, tmax=0.45)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(352, 44, 213)\n",
      "minimum ind across four groups:  84\n",
      "the pattern for this subj is :=====================================\n",
      "1\n",
      "          \n",
      "===================================================================\n"
     ]
    }
   ],
   "source": [
    "args = arguments('early', 'removeevoked', 'False', 'Trgt_Loc_main', 'False' )\n",
    "Grp1, Grp2, Grp3, Grp4, Grps_dt, Grps_avg, smooth_evk, main_ptrn = read_prep_epochs(args)\n",
    "subset = Grp1.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<EpochsFIF  |   93 events (all good), -0.4 - 0.448 sec, baseline [-0.4, 0], ~6.7 MB, data loaded, with metadata,\n",
       " 'pred/left/non': 93>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Subjects with both early and later blocks results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_subj_P1_B = [45, 48, 59, 63]#[8, 43, 45, 48, 59, 63] # Both Early and Later\n",
    "subj_p1 = selected_subj_P1_B.copy()\n",
    "subj_p2 = selected_subj_P2_B.copy()\n",
    "\n",
    "#------------------------------------------------------------------------------#\n",
    "# early ------------- #\n",
    "args = arguments('early', 'removeevoked', 'False', 'Trgt_Loc_prev', 'False' )\n",
    "args.SAVE_RESULT_ROOT = '../../../results/temp_gen/blanca/'\n",
    "#\n",
    "scores_pck_p_e = combine_subj_rand_scores(args, subj_p1, subj_p2)\n",
    "num_tot_subjects = scores_pck_p_e[0].shape[0]\n",
    "#\n",
    "avgP_e = np.mean(scores_pck_p_e[0], axis=0)\n",
    "avgPdiag_e = np.mean(scores_pck_p_e[1], axis=0)\n",
    "#\n",
    "avgPS_e = np.mean(avgP_e, axis=0)\n",
    "avgPSdiag_e = np.mean(avgPdiag_e, axis=0)\n",
    "\n",
    "#------------------------------------------------------------------------------#\n",
    "# later ------------- #\n",
    "args = arguments('later', 'removeevoked', 'False', 'Trgt_Loc_prev', 'False' )\n",
    "args.SAVE_RESULT_ROOT = '../../../results/temp_gen/blanca/'\n",
    "#\n",
    "scores_pck_p_l = combine_subj_rand_scores(args, subj_p1, subj_p2)\n",
    "#\n",
    "avgP_l = np.mean(scores_pck_p_l[0], axis=0)\n",
    "avgPdiag_l = np.mean(scores_pck_p_l[1], axis=0)\n",
    "#\n",
    "avgPS_l = np.mean(avgP_l, axis=0)\n",
    "avgPSdiag_l = np.mean(avgPdiag_l, axis=0)\n",
    "#------------------------------------------------------------------------------#\n",
    "print(avgP_e.shape)\n",
    "print(avgP_l.shape)\n",
    "\n",
    "print(avgPdiag_e.shape)\n",
    "print(avgPdiag_l.shape)\n",
    "\n",
    "print(avgPS_e.shape)\n",
    "print(avgPS_l.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "binned_score_diag, clust_pck = stat_anal(avgPS_e, avgPSdiag_e)\n",
    "\n",
    "plot_scores(avgPS_e)\n",
    "plot_scores_diag(avgPSdiag_e, 0)\n",
    "plot_scores_stat(avgPSdiag_e, clust_pck,str(num_tot_subjects))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_subj_P1_B = [45, 48, 59, 63] # Both Early and Later\n",
    "\n",
    "subj_p1 = selected_subj_P1_B.copy()\n",
    "subj_p2 = selected_subj_P2_B.copy()\n",
    "\n",
    "#------------------------------------------------------------------------------#\n",
    "# early ------------- #\n",
    "args = arguments('early', 'removeevoked', 'False', 'Trgt_Loc_prev', 'False' )\n",
    "args.SAVE_RESULT_ROOT = '../../../results/temp_gen/eCortex/noneFilter_PrePost_decodremoveevoked_bslineFalse_3k/'\n",
    "#\n",
    "scores_pck_p_e = combine_subj_scores(args, subj_p1, subj_p2)\n",
    "num_tot_subjects = scores_pck_p_e[0].shape[0]\n",
    "print('num_tot_subjects is ', num_tot_subjects)\n",
    "# #\n",
    "avgP_e = np.mean(scores_pck_p_e[0], axis=0)\n",
    "avgPdiag_e = np.mean(scores_pck_p_e[1], axis=0)\n",
    "# #\n",
    "# avgPS_e = np.mean(avgP_e, axis=0)\n",
    "# avgPSdiag_e = np.mean(avgPdiag_e, axis=0)\n",
    "\n",
    "#------------------------------------------------------------------------------#\n",
    "# later ------------- #\n",
    "args = arguments('later', 'removeevoked', 'False', 'Trgt_Loc_prev', 'False' )\n",
    "args.SAVE_RESULT_ROOT = '../../../results/temp_gen/eCortex/noneFilter_PrePost_decodremoveevoked_bslineFalse_3k/'\n",
    "#\n",
    "scores_pck_p_l = combine_subj_scores(args, subj_p1, subj_p2)\n",
    "#\n",
    "avgP_l = np.mean(scores_pck_p_l[0], axis=0)\n",
    "avgPdiag_l = np.mean(scores_pck_p_l[1], axis=0)\n",
    "# #\n",
    "# avgPS_l = np.mean(avgP_l, axis=0)\n",
    "# avgPSdiag_l = np.mean(avgPdiag_l, axis=0)\n",
    "\n",
    "#------------------------------------------------------------------------------#\n",
    "print(scores_pck_p_e[0].shape)\n",
    "print(scores_pck_p_e[1].shape)\n",
    "\n",
    "print(avgPdiag_e.shape)\n",
    "print(avgPdiag_l.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "binned_score_diag, clust_pck = stat_anal(avgP_e, avgPdiag_e)\n",
    "\n",
    "plot_scores(avgP_e)\n",
    "plot_scores_diag(avgPdiag_e, 0)\n",
    "plot_scores_stat(avgPdiag_e, clust_pck,str(num_tot_subjects))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_subj_P1_B = [43, 48, 59, 63] # Both Early and Later\n",
    "\n",
    "subj_p1 = selected_subj_P1_B.copy()\n",
    "subj_p2 = selected_subj_P2_B.copy()\n",
    "\n",
    "#------------------------------------------------------------------------------#\n",
    "# early ------------- #\n",
    "args = arguments('early', 'removeevoked', 'False', 'Trgt_Loc_prev', 'False' )\n",
    "args.SAVE_RESULT_ROOT = '../../../results/ERPs/eCortex/'\n",
    "#\n",
    "# scores_pck_p_e = combine_subj_scores(args, subj_p1, subj_p2)\n",
    "\n",
    "p, subset = combine_subj_ERPs(args, subj_p1, subj_p2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def combine_subj_ERPs(args, subj_p1, subj_p2):\n",
    "\n",
    "ERP_p1=[]\n",
    "ERP_p2=[]\n",
    "ind_Grps_dt=4\n",
    "\n",
    "for subj_id in subj_p1:\n",
    "    print('subject id:', subj_id)\n",
    "    main_ptrn = 1\n",
    "    args.subj_num = subj_id\n",
    "\n",
    "    fn_str_sbj='%sBlocks_%sFilter_PrePost_decod%s_bsline%s_%sChann_%s_Subj_%s' \\\n",
    "                %(args.cond_block, args.cond_filter, \\\n",
    "                  args.cond_decoding, args.applyBaseline_bool, \\\n",
    "                  args.occ_channels, args.mtdt_feat, args.subj_num)\n",
    "\n",
    "    fn_str = args.SAVE_RESULT_ROOT + 'ERP_P%s_' %(main_ptrn) + fn_str_sbj\n",
    "\n",
    "\n",
    "    with open(fn_str, 'rb') as f:\n",
    "        erps = pickle.load(f)\n",
    "\n",
    "    erps_avgG1 = np.mean(erps[ind_Grps_dt].copy(), axis=1)\n",
    "    ERP_p1.append(erps_avgG1.copy())\n",
    "\n",
    "\n",
    "for subj_id in subj_p2:\n",
    "    main_ptrn = 2\n",
    "    args.subj_num = subj_id\n",
    "\n",
    "    fn_str_sbj='%sBlocks_%sFilter_PrePost_decod%s_bsline%s_%sChann_%s_Subj_%s' \\\n",
    "                %(args.cond_block, args.cond_filter, \\\n",
    "                  args.cond_decoding, args.applyBaseline_bool, \\\n",
    "                  args.occ_channels, args.mtdt_feat, args.subj_num)\n",
    "\n",
    "    fn_str = args.SAVE_RESULT_ROOT + 'ERP_P%s_' %(main_ptrn) + fn_str_sbj\n",
    "\n",
    "\n",
    "    with open(fn_str, 'rb') as f:\n",
    "        erps = pickle.load(f)\n",
    "\n",
    "    erps_avgG2 = np.mean(erps[ind_Grps_dt].copy(), axis=1)\n",
    "    ERP_p2.append(erps_avgG2.copy())\n",
    "\n",
    "print('p1 subjects:',np.asarray(ERP_p1).shape)\n",
    "print('p2 subjects', np.asarray(ERP_p2).shape)\n",
    "\n",
    "p1=np.asarray(ERP_p1)\n",
    "p2=np.asarray(ERP_p2)\n",
    "\n",
    "\n",
    "if p2.shape[0]==0:\n",
    "    p=p1\n",
    "    print('There was no subjects for pattern 2')\n",
    "elif p1.shape[0]==0:\n",
    "    p=p2\n",
    "    print('There was no subjects for pattern 1')\n",
    "else:\n",
    "    p=np.concatenate((p1, p2), axis=0)\n",
    "\n",
    "\n",
    "subset = erps[0].copy()\n",
    "\n",
    "return p, subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "asdfas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Only subjects with early blocks results\n",
    "-- ignore for now --"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../../../results/temp_gen/blanca/\n",
      "(13, 100, 213)\n",
      "(5, 100, 213)\n",
      "../../../results/temp_gen/blanca/\n",
      "(13, 100, 213)\n",
      "(2, 100, 213)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# early ------------- #\n",
    "# cond_block, cond_decoding, applyBaseline_bool, mtdt_feat, occ_channels)\n",
    "args = arguments('early', 'removeevoked', 'False', 'Trgt_Loc_main', 'False' )\n",
    "scores_pck_p_e = combine_subj_scores(args, selected_subj_P1_E, selected_subj_P2_E)\n",
    "avgP_e = np.mean(scores_pck_p_e[0], axis=0)\n",
    "avgPdiag_e = np.mean(scores_pck_p_e[1], axis=0)\n",
    "avgPS_e = np.mean(avgP_e, axis=0)\n",
    "avgPSdiag_e = np.mean(avgPdiag_e, axis=0)\n",
    "\n",
    "\n",
    "args = arguments('later', 'removeevoked', 'False', 'Trgt_Loc_main', 'False' )\n",
    "scores_pck_p_l = combine_subj_scores(args, selected_subj_P1_L, selected_subj_P2_L)\n",
    "avgP_l = np.mean(scores_pck_p_l[0], axis=0)\n",
    "avgPdiag_l = np.mean(scores_pck_p_l[1], axis=0)\n",
    "avgPS_l = np.mean(avgP_l, axis=0)\n",
    "avgPSdiag_l = np.mean(avgPdiag_l, axis=0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(avgP_e.shape)\n",
    "print(avgP_l.shape)\n",
    "\n",
    "print(avgPdiag_e.shape)\n",
    "print(avgPdiag_l.shape)\n",
    "\n",
    "print(avgPS_e.shape)\n",
    "print(avgPS_l.shape)\n",
    "\n",
    "plot_scores(avgPS_e)\n",
    "plot_scores_diag(avgPSdiag_e, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
